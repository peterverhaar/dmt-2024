{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging and Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Natural Language Processing is an interdisciplinary area of research bringing together insights from fields such as artifical intelligence, computational linguistics, statistics and computer science. The aim of NLP is to enable computers to understand and to process the natural langauges that are spoken and written by human beings. Researchers in the field of NLP have developed sophisticated tools and algorithms for machine translation, document summarisation and sentiment analysis. This notebook explains two specific preprocessing task in NLP: Part of speech tagging and Lemmatisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging\n",
    "\n",
    "Part of speech (POS) taggers are applications which can produce data about the syntactic categories of words. They can create labels such as 'noun', 'adjecvive', 'adverb' or 'determiner' for the words in a text. Using such POS taggers, we can also extract words in specific syntactic categories.\n",
    "\n",
    "Once you have imported the `nltk` library, you can generate such POS tags by making use of the `pos_tag()` method. This method demands a list of words as a parameter. \n",
    "\n",
    "`pos_tag()` is typically used in combination with a word tokenisation method such as `word_tokenize`. The output of this latter function can then be used as input to the `pos_tag()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All => PDT\n",
      "the => DT\n",
      "world => NN\n",
      "is => VBZ\n",
      "a => DT\n",
      "stage => NN\n",
      "and => CC\n",
      "all => PDT\n",
      "the => DT\n",
      "men => NNS\n",
      "and => CC\n",
      "women => NNS\n",
      "merely => RB\n",
      "players => NNS\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "\n",
    "quote = '''All the world is a stage, \n",
    "and all the men and women merely players'''\n",
    "\n",
    "words = word_tokenize(quote)\n",
    "words = remove_punctuation(words)\n",
    "pos = pos_tag(words)\n",
    "\n",
    "for p in pos:\n",
    "    print(p[0] + ' => ' + p[1] )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pos_tag()` methods returns a composite variable with two values. More specifically, it is a data structure that is called a *tuple*. The first value is the word that was tagged and the second value is the POS tag that was assigned to this word. You can access these values individually using square brackets. \n",
    "\n",
    "The meaning of all of the POS tags can be displayed by printing the output of the `nltk.help.upenn_tagset()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print( nltk.help.upenn_tagset() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of codes and their meanings can [also be found online](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation \n",
    "\n",
    "English verbs can be used in the past tense, in the present tense, in the continuous form or in the perfect form, among other forms. These different tenses and forms can evidently make it more difficult to search systematically for occurrences of a specific verb. The same can be the case for nouns. They can be used in the singular and in the plural form. In some situations, we simply want to find all occurrences of a word, regardless of declensions and inflections. In this context, lemmatisation can offer a solution.\n",
    "\n",
    "Lemmatisation is a process in which the conjugated forms of the words that are found in a text are converted into their base dictionary form. This base form is referred to as the lemma. \n",
    "\n",
    "You can lemmatise texts using the  `lemmatize()` method, which is part of the `WordNetLemmatizer` module of the `nltk` library. This method needs to be applied to indivual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n",
      "read\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "print( lemmatiser.lemmatize( 'books' ) )\n",
    "## prints 'book'\n",
    "\n",
    "print( lemmatiser.lemmatize( 'reads' ) )\n",
    "## prints 'read'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It some cases, it can be unclear precisely how words ought to be lemmatised. Certain homonyms may either be verbs or nouns, for instance, and, depending on their usage, they should be lemmatised to different forms. To help the lemmatiser to make such distinctions, we can add a second parameter to indicate the lexical category of the word to be lemmatised. The first statement in the code lemmatises the word 'recording' as a verb, and the second statement as a noun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record\n",
      "recording\n"
     ]
    }
   ],
   "source": [
    "print( lemmatiser.lemmatize( 'recording' , 'v') )\n",
    "## 'record'\n",
    "\n",
    "print( lemmatiser.lemmatize( 'recording' , 'n' ) )\n",
    "## 'recording'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `lemmatize()` method does not use the Penn Treebank codes but the POS codes that have been defined for `wordnet`. It uses 'a' for adjectives, 'v' for verbs, 'n for nouns' and  'r' for adverbs. \n",
    "\n",
    "The code below shows you how you can lemmatise a whole sentence. The code firstly tokenises the words in the sentence that is given using `word_tokenize`. Next, the code generates the POS codes (from Penn Treebank) using `pos_tag`. These codes are then converted into `wordnet` codes used a new function named `ptb_to_wordnet()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we => we\n",
      "are => be\n",
      "such => such\n",
      "stuff => stuff\n",
      "as => as\n",
      "dreams => dream\n",
      "are => be\n",
      "made => make\n",
      "on => on\n",
      "and => and\n",
      "our => our\n",
      "little => little\n",
      "life => life\n",
      "is => be\n",
      "rounded => round\n",
      "with => with\n",
      "a => a\n",
      "sleep => sleep\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "quote = '''We are such stuff as dreams are made on, \n",
    "and our little life is rounded with a sleep'''\n",
    "\n",
    "\n",
    "def ptb_to_wordnet(PTT):\n",
    "\n",
    "    if PTT.startswith('J'):\n",
    "        ## Adjective\n",
    "        return 'a'\n",
    "    elif PTT.startswith('V'):\n",
    "        ## Verb\n",
    "        return 'v'\n",
    "    elif PTT.startswith('N'):\n",
    "        ## Noune\n",
    "        return 'n'\n",
    "    elif PTT.startswith('R'):\n",
    "        ## Adverb\n",
    "        return 'r'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "    \n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "words = word_tokenize(quote)\n",
    "words = remove_punctuation(words)\n",
    "\n",
    "pos = nltk.pos_tag(words)\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    word = word.lower()\n",
    "    posTag = ptb_to_wordnet( pos[i][1] )\n",
    "    \n",
    "    if re.search( r'\\w+' , posTag , re.IGNORECASE ):\n",
    "        lemma = lemmatiser.lemmatize( words[i] , posTag )\n",
    "        print( f'{word} => {lemma}' )\n",
    "    else:\n",
    "        print( f'{word} => {word}' )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.1\n",
    "\n",
    "Create a list containing the unique adjectives that are occur in *Pride and Prejudice*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "such => 285\n",
      "other => 208\n",
      "much => 204\n",
      "little => 179\n",
      "own => 179\n",
      "good => 172\n",
      "more => 155\n",
      "great => 141\n",
      "young => 126\n",
      "last => 118\n",
      "many => 115\n",
      "first => 109\n",
      "dear => 95\n",
      "sure => 92\n",
      "happy => 80\n",
      "few => 71\n",
      "same => 68\n",
      "next => 67\n",
      "least => 60\n",
      "better => 59\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "path = os.path.join('Corpus','PrideAndPrejudice.txt')\n",
    "\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "pos = pos_tag(words)\n",
    "\n",
    "adjectives = []\n",
    "adj_codes = ['JJ','JJR','JJS']\n",
    "\n",
    "for p in pos:\n",
    "    if p[1] in adj_codes:\n",
    "        adjectives.append(p[0])\n",
    "        \n",
    "freq = Counter(adjectives)\n",
    "\n",
    "for word,count in freq.most_common(20):\n",
    "    print(f'{word} => {count}')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.2\n",
    "\n",
    "Stephen King is [reputed to have said](https://www.goodreads.com/quotes/430289-i-believe-the-road-to-hell-is-paved-with-adverbs) that “the road to hell is paved with adverbs\", and many style guides similarly give writers the advice to avoid adverbs, especially those ending in '-ly'. \n",
    "\n",
    "Can you calculate, for each text in the corpus, the number of adverb ending in '-ly', measured as a percentage of the total number of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sonnet116.txt\n",
      "0 adverbs ending in '-ly' in total.\n",
      "This is 0.0% of all the words \n",
      "\n",
      "Ullyses.txt\n",
      "3179 adverbs ending in '-ly' in total.\n",
      "This is 0.012% of all the words \n",
      "15 most frequent adverbs:\n",
      "only => 212\n",
      "slowly => 71\n",
      "molly => 70\n",
      "simply => 57\n",
      "quickly => 54\n",
      "really => 49\n",
      "milly => 42\n",
      "suddenly => 39\n",
      "gently => 38\n",
      "lovely => 36\n",
      "probably => 34\n",
      "quietly => 33\n",
      "softly => 33\n",
      "loudly => 29\n",
      "certainly => 29\n",
      "\n",
      "BraveNewWorld.txt\n",
      "1203 adverbs ending in '-ly' in total.\n",
      "This is 0.0195% of all the words \n",
      "15 most frequent adverbs:\n",
      "only => 89\n",
      "suddenly => 81\n",
      "really => 50\n",
      "slowly => 30\n",
      "actually => 21\n",
      "simply => 13\n",
      "absolutely => 11\n",
      "simultaneously => 10\n",
      "merely => 10\n",
      "beastly => 10\n",
      "finally => 10\n",
      "carefully => 9\n",
      "particularly => 9\n",
      "hardly => 9\n",
      "perfectly => 8\n",
      "\n",
      "PrideandPrejudice.txt\n",
      "1891 adverbs ending in '-ly' in total.\n",
      "This is 0.0157% of all the words \n",
      "15 most frequent adverbs:\n",
      "only => 190\n",
      "really => 89\n",
      "certainly => 70\n",
      "immediately => 61\n",
      "perfectly => 48\n",
      "hardly => 46\n",
      "scarcely => 44\n",
      "merely => 33\n",
      "particularly => 33\n",
      "instantly => 31\n",
      "exactly => 30\n",
      "highly => 28\n",
      "exceedingly => 27\n",
      "easily => 26\n",
      "wholly => 26\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "\n",
    "directory = os.path.join('Corpus')\n",
    "files = os.listdir(directory)\n",
    "\n",
    "for file in files:\n",
    "    print(f\"\\n{file}\")\n",
    "    path = os.path.join(directory,file)\n",
    "    \n",
    "    full_text = ''\n",
    "    with open( path , encoding = 'utf-8') as file:\n",
    "        full_text = file.read()\n",
    "\n",
    "    words = word_tokenize(full_text.lower())\n",
    "    words = remove_punctuation(words)\n",
    "    nr_words = len(words)\n",
    "    pos = pos_tag(words)\n",
    "\n",
    "    adjectives = []\n",
    "    adj_codes = ['RB','RBR','RBS']\n",
    "\n",
    "    ly_adverbs = 0\n",
    "    for p in pos:\n",
    "        if p[1] in adj_codes and p[0][-2:].strip() == 'ly':\n",
    "            adjectives.append(p[0])\n",
    "            ly_adverbs += 1\n",
    "\n",
    "    freq = Counter(adjectives)\n",
    "        \n",
    "    print(f\"{ly_adverbs} adverbs ending in '-ly' in total.\")\n",
    "    print(f\"This is {round(ly_adverbs/nr_words,4)}% of all the words \")\n",
    "    \n",
    "    number = 15\n",
    "    if ly_adverbs>0:\n",
    "        print(f\"{number} most frequent adverbs:\")\n",
    "        for word,count in freq.most_common(number):\n",
    "            print(f'{word} => {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3\n",
    "\n",
    "Which text in the corpus has the highest number of modal verbs? The Penn Treebank code for 'modal auxialiaries' is MD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sonnet116.txt\n",
      "0 modal verbs.\n",
      "\n",
      "Ullyses.txt\n",
      "2018 modal verbs.\n",
      "10 most frequent modal verbs:\n",
      "would => 382\n",
      "will => 340\n",
      "could => 309\n",
      "can => 276\n",
      "must => 221\n",
      "might => 176\n",
      "may => 91\n",
      "should => 82\n",
      "shall => 66\n",
      "ought => 61\n",
      "\n",
      "BraveNewWorld.txt\n",
      "518 modal verbs.\n",
      "10 most frequent modal verbs:\n",
      "would => 116\n",
      "could => 116\n",
      "can => 68\n",
      "should => 47\n",
      "must => 43\n",
      "will => 37\n",
      "might => 34\n",
      "ought => 24\n",
      "may => 18\n",
      "shall => 15\n",
      "\n",
      "PrideandPrejudice.txt\n",
      "2892 modal verbs.\n",
      "10 most frequent modal verbs:\n",
      "could => 522\n",
      "would => 468\n",
      "will => 413\n",
      "can => 320\n",
      "must => 308\n",
      "should => 252\n",
      "might => 200\n",
      "may => 194\n",
      "shall => 162\n",
      "ought => 45\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "\n",
    "directory = os.path.join('Corpus')\n",
    "files = os.listdir(directory)\n",
    "\n",
    "for file in files:\n",
    "    print(f\"\\n{file}\")\n",
    "    path = os.path.join(directory,file)\n",
    "    \n",
    "    full_text = ''\n",
    "    with open( path , encoding = 'utf-8') as file:\n",
    "        full_text = file.read()\n",
    "\n",
    "    words = word_tokenize(full_text.lower())\n",
    "    words = remove_punctuation(words)\n",
    "    nr_words = len(words)\n",
    "    pos = pos_tag(words)\n",
    "    \n",
    "    modal_verbs = []\n",
    "\n",
    "    for p in pos:\n",
    "        if p[1] == 'MD' and len(p[0])>2:\n",
    "            modal_verbs.append(p[0])\n",
    "\n",
    "    freq = Counter(modal_verbs)\n",
    "        \n",
    "    print(f\"{len(modal_verbs)} modal verbs.\")\n",
    "\n",
    "    number = 10\n",
    "    if len(modal_verbs)>0:\n",
    "        print(f\"{number} most frequent modal verbs:\")\n",
    "        for word,count in freq.most_common(number):\n",
    "            print(f'{word} => {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.4\n",
    "\n",
    "Extract all the sentences from *BraveNewWorld.txt* that contain an adjective in the superlative form.  Write these sentences into a file named 'sentences.txt'. The code for the words in these category is 'JJS'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m( path , encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      9\u001b[0m     full_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 11\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m(full_text)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m     14\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(sentence)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "\n",
    "path = os.path.join( 'Corpus','BraveNewWorld.txt')\n",
    "\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words = remove_punctuation(words)\n",
    "    pos = pos_tag(words)\n",
    "    \n",
    "    adj = []\n",
    "    for p in pos:\n",
    "        if p[1] == 'JJS':\n",
    "            adj.append(p[0])\n",
    "            \n",
    "    if len(adj)>0:\n",
    "        print(f\"{sentence} [{'|'.join(adj)}]\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise11.5\n",
    "\n",
    "Extract all the sentences from *Ullyses.txt* containing a form of the verb 'to see', in all tenses and conjugations and excepting the infitive form. In other words, extract sentences containing forms such as 'seen', 'saw' or 'seeing', but not 'see'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m( path , encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     12\u001b[0m     full_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 14\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m(full_text)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m     18\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(sentence\u001b[38;5;241m.\u001b[39mlower())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from text_mining import *\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "path = os.path.join( 'Corpus','Ullyses.txt')\n",
    "\n",
    "full_text = ''\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    words = remove_punctuation(words)\n",
    "\n",
    "    pos = nltk.pos_tag(words)\n",
    "    \n",
    "    hits = []\n",
    "\n",
    "    for i,word in enumerate(words):\n",
    "        word = word.lower()\n",
    "        posTag = ptb_to_wordnet( pos[i][1] )\n",
    "\n",
    "        if re.search( r'\\w+' , posTag , re.IGNORECASE ):\n",
    "            lemma = lemmatiser.lemmatize( words[i] , posTag )\n",
    "            if lemma == 'see':\n",
    "                hits.append(word)\n",
    "        else:\n",
    "            if word == 'see':\n",
    "                hits.append(word)\n",
    "                \n",
    "    if len(hits)>0:\n",
    "        print(f\"{sentence}\\n---\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.6\n",
    "\n",
    "From *Ullyses.txt* , extract all sentnces containing the following combinations of categories: \n",
    "\n",
    "* Article - adverb - adjective - noun \n",
    "\n",
    "These categorties can be asigned the following codes:\n",
    "\n",
    "* Article: DT\n",
    "* Adverb: RB, RBR or RBS\n",
    "* Adjective: JJ, JJR or JJS\n",
    "* Noun: NN, NNP, NNPS or NNS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m join \n\u001b[1;32m     10\u001b[0m path \u001b[38;5;241m=\u001b[39m join( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorpus\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUllyses.txt\u001b[39m\u001b[38;5;124m'\u001b[39m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tdm'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize , sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tdm import *\n",
    "\n",
    "\n",
    "from os.path import join \n",
    "\n",
    "path = join( 'Corpus','Ullyses.txt' )\n",
    "with open( path , encoding = 'utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    \n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub(r'\\n',' ',sentence)\n",
    "    words = word_tokenize(sentence)\n",
    "    words = remove_punctuation(words)\n",
    "    pos = pos_tag(words)\n",
    "    \n",
    "    tagged_sentence = ''\n",
    "\n",
    "    for p in pos:\n",
    "        tagged_sentence += p[1] + ' '\n",
    "\n",
    "    if re.search( r'DT RB JJ NN' , tagged_sentence):\n",
    "        print(f\"{sentence}\\n---\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
