{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the context of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances\n",
    "\n",
    "Lists of frequent words can be very useful: they can help to clarify the main concerns or the themes of a text. To examine *how* words are used in a text, it can be useful, additionally, to create a concordance. In a concordance, all the occurrences of a given search term are listed in combination with words that occur before and after this term. Such resources are sometimes referred to as *keyword in context* lists (KWIC). \n",
    "\n",
    "The `nltk` package contains a method named `concordance()`. To work with this method, you firstly need to create an instance of the `Text` class. This class is part of the `nltk.text` module. Such a `Text` object can be initialised using a list with all the tokens of a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.text import Text\n",
    "import os\n",
    "\n",
    "path = os.path.join('Corpus','PrideAndPrejudice.txt')\n",
    "\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "tokens = word_tokenize(full_text)\n",
    "novel = Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the `Text` object is given the name `novel`. \n",
    "\n",
    "Once you have created such an object, you can use its `concordance()` method. You can supply three parameters: \n",
    "\n",
    "1. A search term.\n",
    "2. A `width` parameter, specifying the extent of the context. With this parameter, you indicate the number of characters before and after the word whose context you want to see. \n",
    "3. A `lines` parameter, which specifies the number of results. \n",
    "\n",
    "Out of these parameters, only the first one is mandatory. When you leave out the last two parameters, the method will work with its default values: a width of 70 (35 characters before and 35 characters after the search term) and 25 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 66 matches:\n",
      "month . Happiness in marriage is entirely a matter\n",
      "ng of their supposed marriage , and planning his h\n",
      " well disposed of in marriage . This gallantry was\n",
      "probability of their marriage was exceedingly agre\n",
      "the felicity which a marriage of true affection co\n",
      "hat another offer of marriage may ever be made you\n",
      "for happiness in the marriage state . If therefore\n",
      "made you an offer of marriage . Is it true ? '' El\n",
      "llâ€”and this offer of marriage you have refused ? '\n",
      "using every offer of marriage in this way , you wi\n"
     ]
    }
   ],
   "source": [
    "novel.concordance('marriage' , width = 50 , lines = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `concordance()` method that is defined in `nltk`, the width of the context is defined using a specific number of characters. When you work with such a fixed number of characters, the search term can be shown at the same position on each line, resulting in a keyword-in-context list with a nice and orderly appearance.\n",
    "\n",
    "The downside of this approach is that the various lines may contain incomplete words. Which you indicate that the size of the context must be set at 20 characters before and after the term, the code simply removes all characters preceding or following the twentieth character. \n",
    "\n",
    "The cell below contains a definition of a method which can create a somwhat different type of concordance. In this method, named `concordance_words()`, the width of the context is specified on the basis of words rather characters. When you supply the number 10 as the value for the parameter defining the width, you will receive all occurrences of the search term, together with 5 words before and 5 words after this search term. The method demands a sting as input. This string can be the full text of a novel. \n",
    "\n",
    "The results are returned as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from text_mining import *\n",
    "\n",
    "def concordance_word( text, regex , width = 10 ):\n",
    "\n",
    "    concordance = []\n",
    "    distance = math.floor( width /2 )\n",
    "\n",
    "    segment_length = 0\n",
    "\n",
    "    words = word_tokenize( text )\n",
    "    words = remove_punctuation( words )\n",
    "    i = 0\n",
    "    for w in words:\n",
    "        if re.search( regex , w , re.IGNORECASE ):\n",
    "            match = ''\n",
    "            for x in range( i - distance , ( i + distance ) + 1 ):\n",
    "                if x >= 0 and x < len(words):\n",
    "                    if len(words[x]) >= 0:\n",
    "                        match += words[x] + ' '\n",
    "            concordance.append( match )\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return concordance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains an illustration of how you can use this method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 67 ocurrences of the word \"marriage\".\n",
      "Here are the first 5 occurrences:\n",
      "\n",
      "\n",
      "studying his character for a twelvemonth Happiness in marriage is entirely a matter of chance If the \n",
      "\n",
      "disliking her guest by talking of their supposed marriage and planning his happiness in such an alliance \n",
      "\n",
      "all in due time well disposed of in marriage This gallantry was not much to the taste \n",
      "\n",
      "her to understand that the probability of their marriage was exceedingly agreeable to her Elizabeth however did \n",
      "\n",
      "very house in all the felicity which a marriage of true affection could bestow and she felt \n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join('Corpus','PrideAndPrejudice.txt')\n",
    "\n",
    "with open( path, encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "    \n",
    "fragments = concordance_word( full_text , r'marriage' , 16)\n",
    "\n",
    "print( f'There are {len(fragments)} ocurrences of the word \"marriage\".')\n",
    "\n",
    "number_of_results = 5\n",
    "\n",
    "print( f'Here are the first {number_of_results} occurrences:\\n\\n')\n",
    "for f in fragments[:number_of_results]:\n",
    "    print( f'{f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the definition of `concordance_word()`, the method searches for occurrences of the supplied search term as a [regular expression](https://cdsleiden.github.io/python-tutorial/notebooks/9%20Regular_expressions.html). The second parameter of this method can also be a more complicated regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best education can overcome And your defect is a propensity to hate every body And yours he replied with a smile is wilfully to \n",
      "\n",
      "there is not a bit of fish to be got Lydia my love ring the bell I must speak to Hill this moment It is \n",
      "\n",
      "of him to write to you at all and very hypocritical I hate such false friends Why could not he keep on quarrelling with you \n",
      "\n",
      "is that we are very different sort of men and that he hates me This is quite shocking deserves to be publicly disgraced Some time \n",
      "\n",
      "misfortune of all find a man agreeable whom one is determined to hate not wish me such an evil When the dancing recommenced however and \n",
      "\n",
      "I shall chuse to attribute it to your wish of increasing my love by suspense according to the usual practice of elegant females I do \n",
      "\n",
      "Collins was not left long to the silent contemplation of his successful love for Bennet having dawdled about in the vestibule to watch for the \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fragments = concordance_word(full_text , r'(\\bhates?\\b)|(\\bloves?\\b)' , 25)\n",
    "\n",
    "for f in fragments[15:22]:\n",
    "    print( f'{f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation analysis\n",
    "\n",
    "Collocation analyses focus on the words that are used in the vicinity of a provided search term. It may be viewed as an extension of the principle underlying the creation of concordances. To perform a collocation analysis, we can look at the environments of a search term through a 'window' consisting of a given number of words. The words that are used in this context can obviously be counted. The aim of a collocation analysis is to identify the words that are used most frequently in the neighbourhood of a given word. \n",
    "\n",
    "Such collocation analyses can be carried out using the `collocation()` method that is defined below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation( text , regex , width ):\n",
    "\n",
    "    freq_c = dict()\n",
    "    distance = math.floor(width/2)\n",
    "\n",
    "    sentences = sent_tokenize( text )\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        words = word_tokenize( sentence )\n",
    "        words = remove_punctuation(words)\n",
    "\n",
    "        for i,w in enumerate(words):\n",
    "            if re.search( regex , w , re.IGNORECASE ):\n",
    "                index_regex = i \n",
    "\n",
    "                for x in range( i - distance , i + distance ):\n",
    "                    if x >= 0 and x < len(words) and words[x].lower() != words[index_regex].lower():\n",
    "                        if len(words[x]) > 0:\n",
    "                            word = words[x].lower()\n",
    "                            freq_c[ word ] = freq_c.get( word , 0 ) + 1\n",
    "            \n",
    "    return freq_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are the same as those of the `concordance_word()` method: \n",
    "\n",
    "1. The text that needs to be analysed.\n",
    "2. A search term, which will be treated as a regular expression.\n",
    "3. A number representing the width of the context (or, ot be more precise: the number of words). \n",
    "\n",
    "This function returns a dictionary listing all the words found near the search term that is provided, together with the frequencies of these words.  \n",
    "\n",
    "The code below makes use of the function `sortedByValue()` which can sort a dictionary by value, and the list of stopwords from `nltk` to remove the function words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_words = collocation( full_text , r'marriage' , 20)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "nearby_words_sorted = sortedByValue( nearby_words , ascending = False)\n",
    "\n",
    "for word in list( nearby_words_sorted.keys() ):\n",
    "    freq = nearby_words_sorted[word]\n",
    "    if word not in stopwords and freq > 2:\n",
    "        print( f'{word} => {freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooccurrence\n",
    "\n",
    "Once you have established that two specific words are often used in combination, you can begin to study specific combinations of words in more detail using the `cooccurrence()` method that is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence( text , word1 , word2 , width ):\n",
    "    \n",
    "    relevant_sentences = []\n",
    "    \n",
    "    text = re.sub( '\\s+' , ' ' , text )\n",
    "    sentences = sent_tokenize( text )\n",
    "\n",
    "    for s in sentences:\n",
    "        if re.search( r'\\b' + word1 + r'\\b' , s , re.IGNORECASE ) and re.search( r'\\b' + word2 + r'\\b' , s , re.IGNORECASE ):\n",
    "\n",
    "            words = word_tokenize(s)\n",
    "            word1_indexes = []\n",
    "            word2_indexes = []\n",
    "            \n",
    "            for i,w in enumerate(words):\n",
    "                if re.search( r'\\b' + word1 + r'\\b' , w , re.IGNORECASE ):\n",
    "                    word1_indexes.append(i)\n",
    "                elif re.search( r'\\b' + word2 + r'\\b' , w , re.IGNORECASE ):\n",
    "                    word2_indexes.append(i)\n",
    "\n",
    "            if word1_indexes[0] > word2_indexes[0]:\n",
    "                difference = word1_indexes[0] - word2_indexes[0]\n",
    "            else:\n",
    "                difference = word2_indexes[0] - word1_indexes[0]\n",
    "\n",
    "            if difference <= width:\n",
    "                relevant_sentences.append(s)\n",
    "    return relevant_sentences\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The useage of the method is as follows:\n",
    "    \n",
    "* As the first parameter, you mus provide the full text that you want to analyse, as a single string.\n",
    "* As the second and the third parameter, you need to mention the two words that you are interested in. \n",
    "* How close should these two words be? The fourth parameter specifies the number of words that are allowed in between the two words you focus on.  \n",
    "\n",
    "The method `cooccurrence()` returns all the sentences containing the two words that you focus on. The distance (measured in number of words) will not be greater than the width that you specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = cooccurrence( full_text , 'marriage' , 'lydia' , 10 )\n",
    "\n",
    "for s in sentences:\n",
    "    print( f'{s}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.1\n",
    "\n",
    "Create a concordance for the word 'savage' in the novel *Brave New World*. You can find the full text in the 'Corpus' folder. Work with a width of 50 characters (i.e. 25 characters before and 25 characters after this search term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 201 matches:\n",
      "e to go to one of the Savage Reservations with him\n",
      "lways wanted to see a Savage Reservation . ' 'But \n",
      "se I do want to see a Savage Reservation . ' * * *\n",
      " they would be in the Savage Reservation . Not mor\n",
      "ad ever been inside a Savage Reservation . As an A\n",
      "e is no escape from a Savage Reservation . ' The w\n",
      "d to the sullen young savage . 'Funny , I expect .\n",
      " more the men 's deep savage affirmation of their \n",
      "heek . 'Turned into a savage , ' she shouted . 'Ha\n",
      "ather ' of this young savage must be . 'Would you \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.text import Text\n",
    "import os\n",
    "\n",
    "path = os.path.join('Corpus','BraveNewWorld.txt')\n",
    "\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "tokens = word_tokenize(full_text)\n",
    "novel = Text(tokens)\n",
    "\n",
    "novel.concordance('savage' , width = 50 , lines = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.2. \n",
    "\n",
    "Create a concordance for the word 'soma' in the novel *Brave New World*. This time, work with a width of 20 words (i.e. 10 words before and 10 words after this search term). Display the first 15 occurrences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m( path, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     10\u001b[0m     full_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 12\u001b[0m fragments \u001b[38;5;241m=\u001b[39m \u001b[43mconcordance_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(fragments)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ocurrences of the word \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoma\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m number_of_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Leiden/1st semester program/Digital Media Technology/peterverhaar-textmining_with_python-040eb72/text_mining.py:31\u001b[0m, in \u001b[0;36mconcordance_word\u001b[0;34m(text, regex, width)\u001b[0m\n\u001b[1;32m     27\u001b[0m distance \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mfloor( width \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m )\n\u001b[1;32m     29\u001b[0m segment_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 31\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m( text )\n\u001b[1;32m     32\u001b[0m words \u001b[38;5;241m=\u001b[39m remove_punctuation( words )\n\u001b[1;32m     33\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import os\n",
    "\n",
    "from text_mining import *\n",
    "\n",
    "path = os.path.join('Corpus','BraveNewWorld.txt')\n",
    "\n",
    "with open( path, encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "    \n",
    "fragments = concordance_word( full_text , r'soma' , 20)\n",
    "\n",
    "print( f'There are {len(fragments)} ocurrences of the word \"soma\".')\n",
    "\n",
    "number_of_results = 15\n",
    "\n",
    "print( f'Here are the first {number_of_results} occurrences:\\n\\n')\n",
    "for f in fragments[:number_of_results]:\n",
    "    print( f'{f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.3\n",
    "\n",
    "In *Ullyses*, which words are used most frequently in the vicinity of the word 'father'? Consider 8 words before and 8 words after all the occurrences of this specific name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m( path, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     13\u001b[0m     full_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 15\u001b[0m nearby_words \u001b[38;5;241m=\u001b[39m \u001b[43mcollocation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfather\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m     18\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Leiden/1st semester program/Digital Media Technology/peterverhaar-textmining_with_python-040eb72/text_mining.py:56\u001b[0m, in \u001b[0;36mcollocation\u001b[0;34m(text, regex, width)\u001b[0m\n\u001b[1;32m     53\u001b[0m freq_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     54\u001b[0m distance \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mfloor(width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m( text )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m     60\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize( sentence )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "from text_mining import *\n",
    "import os\n",
    "\n",
    "def sortedByValue( dict , ascending = True ):\n",
    "    if ascending: \n",
    "        return {k: v for k, v in sorted(dict.items(), key=lambda item: item[1])}\n",
    "    else:\n",
    "        return {k: v for k, v in reversed( sorted(dict.items(), key=lambda item: item[1]))}\n",
    "\n",
    "path = os.path.join('Corpus','Ullyses.txt')\n",
    "\n",
    "with open( path, encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "    \n",
    "nearby_words = collocation( full_text , r'father' , 20)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "nearby_words_sorted = sortedByValue( nearby_words , ascending = False)\n",
    "\n",
    "for word in list( nearby_words_sorted.keys() ):\n",
    "    freq = nearby_words_sorted[word]\n",
    "    if word not in stopwords and freq > 2:\n",
    "        print( f'{word} => {freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.4\n",
    "\n",
    "Find all the sentences in *Ullyses* that contain the words 'book' and 'read'. Make sure that, in these sentences, there are no more than 10 words in between these two words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m( path, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m     full_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 10\u001b[0m cooccurrences \u001b[38;5;241m=\u001b[39m \u001b[43mcooccurrence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mread\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fragment \u001b[38;5;129;01min\u001b[39;00m cooccurrences:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(fragment)\n",
      "File \u001b[0;32m~/Documents/Leiden/1st semester program/Digital Media Technology/peterverhaar-textmining_with_python-040eb72/text_mining.py:82\u001b[0m, in \u001b[0;36mcooccurrence\u001b[0;34m(text, word1, word2, width)\u001b[0m\n\u001b[1;32m     79\u001b[0m relevant_sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     81\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m , text )\n\u001b[0;32m---> 82\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m( text )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch( \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m word1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m , s , re\u001b[38;5;241m.\u001b[39mIGNORECASE ) \u001b[38;5;129;01mand\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch( \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m word2 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m , s , re\u001b[38;5;241m.\u001b[39mIGNORECASE ):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "from text_mining import *\n",
    "import os\n",
    "\n",
    "\n",
    "path = os.path.join('Corpus','Ullyses.txt')\n",
    "\n",
    "with open( path, encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "    \n",
    "cooccurrences = cooccurrence( full_text , 'book' , 'read' , 10 )\n",
    "\n",
    "for fragment in cooccurrences:\n",
    "    print(fragment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
