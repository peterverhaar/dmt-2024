{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Word frequencies\n",
    "\n",
    "The notebook on *Tokenisation* explained how we can divide a text into its separate words. Once we have managed to do this, we can begin to count the **types** (i.e. the occurrences of the unique words). \n",
    "\n",
    "## The Counter object\n",
    " \n",
    "The task of *counting* can be performed very effectively in Python using the `Counter` object in the `collections` package. \n",
    "\n",
    "To demonstarte how this object works, we firstly create a list of words. As was discussed earlier, such a list of words can be created using the word_tokenize method from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from text_mining import remove_punctuation\n",
    "\n",
    "quote = '''It was the best of times, it was the worst of times, \n",
    "it was the age of wisdom, it was the age of foolishness, \n",
    "it was the epoch of belief, it was the epoch of incredulity'''\n",
    "\n",
    "words = remove_punctuation(word_tokenize(quote.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the cell above also make use of the function `remove_punctuation`, which was discussed earlier. The function is also defined in a module named `text_mining`. This module is imported on the second line. \n",
    "\n",
    "After you have run the cell above, the `list` named 'words' contains the individual words in the quote that was given. As you can see, certain words are used repeatedly, such as 'times', 'age' and 'it'. \n",
    "\n",
    "To work with the `Counter` object it firstly needs to be imported. The object is part of the `collections` package.  \n",
    "\n",
    "The `Counter` can be initiated with a Python list, as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_frequencies = Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable named `word_frequencies`, created in the cell above, is an instantiation of the `Counter` object. It has been created with all the words (or tokens) in the list named `words`.\n",
    "\n",
    "It is a varibale that contains multiple items. Each item consists of two components: \n",
    "1. The word (or more specifically: the type) \n",
    "2. The number of times the word occurs in the original text fragment. \n",
    "\n",
    "The variable `word_frequencies` is in fact a specific type of [dictionary](https://cdsleiden.github.io/python-tutorial/notebooks/6%20Dictionaries.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'it': 6, 'was': 6, 'the': 6, 'of': 6, 'times': 2, 'age': 2, 'epoch': 2, 'best': 1, 'worst': 1, 'wisdom': 1, 'foolishness': 1, 'belief': 1, 'incredulity': 1})\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, you can also add more words to the Counter object using the `update()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'it': 12, 'was': 12, 'the': 12, 'of': 12, 'times': 4, 'age': 4, 'epoch': 4, 'best': 2, 'worst': 2, 'wisdom': 2, 'foolishness': 2, 'belief': 2, 'incredulity': 2})\n"
     ]
    }
   ],
   "source": [
    "next_sentence = '''It was the season of Light, \n",
    "it was the season of Darkness, it was the spring of hope, \n",
    "it was the winter of despair.'''\n",
    "\n",
    "words = remove_punctuation(word_tokenize(quote.lower()))\n",
    "word_frequencies.update(words)\n",
    "\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Counter` object has a method named `most_common()` which can be used to list the items with the highest frequencies. The parameter of this method specifies the number of items to be shown.\n",
    "\n",
    "If you iterate across all the items returned by `most_common()`, two values are available in each iteration: (1) the word (or type) and (2) the frequency of this token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it => 12\n",
      "was => 12\n",
      "the => 12\n",
      "of => 12\n",
      "times => 4\n",
      "age => 4\n",
      "epoch => 4\n",
      "best => 2\n",
      "worst => 2\n",
      "wisdom => 2\n"
     ]
    }
   ],
   "source": [
    "for word, count in word_frequencies.most_common(10):\n",
    "    print(f'{word} => {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of a specific word in a text can be found by mentioned this word in square brackets after the variable named `word_frequencies`, which, as mentioned, is an instance of the `Counter` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies['times'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9.1.\n",
    "\n",
    "The 'Corpus' folder contains the full text of the novel *Pride and Prejudice*. Use this text file to answer the following questions. \n",
    "\n",
    "* How many words does the novel contain?\n",
    "* What are the 25 most frequent words of the novel?\n",
    "* How many times does the novel contain the token 'Darcy'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel contains 120081 words.\n",
      "to => 4076\n",
      "the => 4056\n",
      "of => 3605\n",
      "and => 3371\n",
      "her => 2131\n",
      "I => 2020\n",
      "a => 1874\n",
      "was => 1842\n",
      "in => 1783\n",
      "that => 1520\n",
      "not => 1506\n",
      "she => 1378\n",
      "it => 1273\n",
      "be => 1230\n",
      "his => 1191\n",
      "had => 1148\n",
      "you => 1137\n",
      "as => 1132\n",
      "he => 1096\n",
      "for => 1029\n",
      "with => 1017\n",
      "have => 836\n",
      "is => 835\n",
      "him => 761\n",
      "at => 737\n",
      "The word \"Darcy\" occurs 415 times.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        if w.isalnum():\n",
    "            new_list.append( w )\n",
    "    return new_list\n",
    "\n",
    "text_file = open('Corpus/PrideAndPrejudice.txt',encoding='utf-8')\n",
    "full_text = text_file.read()\n",
    "\n",
    "words = nltk.word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "\n",
    "print(f\"The novel contains {len(words)} words.\")\n",
    "\n",
    "freq = Counter(words)\n",
    "\n",
    "for word,count in freq.most_common(25):\n",
    "    print(f\"{word} => {count}\")\n",
    "    \n",
    "keyword = 'Darcy'\n",
    "print(f'The word \"{keyword}\" occurs {freq[keyword]} times.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "In normal texts, lexical categories such pronouns, prepositions or articles (words such as 'the', 'of' and 'it') are generally very frequent. Such words are sometimes referred to as 'function words'. Words such as these are important for producting grammatically correct sentences, but they mostly have little expessive value independently.\n",
    "\n",
    "If you are interested in studying the actual contents or the semantics of a text, it can be useful to remove such frequently used function words. \n",
    "\n",
    "If you want to do this, one option is to make use of the list of stopwords available as part of the `nltk` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the stopwords are disregarded in the frequency counts, we can write a new version of the `remove_punctuation()` function. The `if` keyword in the function `remove_punctuation_and_stopwords()` is followed by two conditions: 1. The word should contain an alphanumerical character and \n",
    "2. The word should not be on the list of stopwords.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_and_stopwords(words):\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        if w.isalnum() and w not in stopwords:\n",
    "            new_list.append( w )\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains a revision of the code that was discussed earlier for calculating the word frequencies. This revised version removes both stopwords and punctuation, based on the function named `remove_punctuation_and_stopwords()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times => 2\n",
      "age => 2\n",
      "epoch => 2\n",
      "best => 1\n",
      "worst => 1\n",
      "wisdom => 1\n",
      "foolishness => 1\n",
      "belief => 1\n",
      "incredulity => 1\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from text_mining import remove_punctuation\n",
    "\n",
    "quote = '''It was the best of times, it was the worst of times, \n",
    "it was the age of wisdom, it was the age of foolishness, \n",
    "it was the epoch of belief, it was the epoch of incredulity'''\n",
    "\n",
    "words = word_tokenize(quote.lower())\n",
    "words = remove_punctuation_and_stopwords(words)\n",
    "\n",
    "word_frequencies = Counter(words)\n",
    "\n",
    "for word, count in word_frequencies.most_common(10):\n",
    "    print(f'{word} => {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to visualise a given dictionary with a word frequencies into a word cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "wordcloud = WordCloud( background_color=\"white\",  width=1500,height=1000, max_words= 100,relative_scaling=1,normalize_plurals=False).generate_from_frequencies(word_frequencies)\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code above produced an error message, this may be caused by the fact that `wordcloud` module has not been inestalled yet on your computer. In that case, you may need to install the `wordcloud` module using the commands in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the hashes in the next two lines!\n",
    "\n",
    "#import sys\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code that was explained in this notebook tokenises texts using the `word_tokenize()` method from `nltk`. As was explained, this method treats all the punction marks as separate tokens. Arguably, such occurrences of semi-colons, dots, commas and quotes should be ignored when we calculate the frequencies of words. We can do this by using a method which removes the tokens that don't contain any alphanumerical characters. \n",
    "\n",
    "In the case of tokens containing the genitival 's' (e.g. as in \"the people's choice\" or \"the child's parents\", `nltk` aims to separate the final 's' from the rest of the word. As a result of this, the character 's' will likewise be viewed and processed as a separate token. \n",
    "\n",
    "Something similar happens with the various parts of contracted verb forms. `nltk` aims to separate the stem from the rest of token in forms such as \"don't\", \"i'm\" and \"weren't\", but this results in tokens such as \"'nt\", \"'m\" and \"'t\", respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "'s\n",
      "a\n",
      "corrupting\n",
      "thing\n",
      "to\n",
      "live\n",
      "one\n",
      "'s\n",
      "real\n",
      "life\n",
      "in\n",
      "secret\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "quote = \"It's a corrupting thing to live one's real life in secret.\"\n",
    "words = word_tokenize(quote)\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question whether words in the genitival forms and contracted verb forms must be counted separately is of course open to debate. It can be useful to know, however, that such 'incomplete' tokens are also on `nltk`'s list of stopwords. When you filter the list of tokens using these stopwords, you will also get rid of seemingly nonsensical tokens such as \"'m\", \"'s\" and \"'t\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrupting\n",
      "thing\n",
      "live\n",
      "one\n",
      "real\n",
      "life\n",
      "secret\n"
     ]
    }
   ],
   "source": [
    "quote = \"It is a corrupting thing to live one's real life in secret.\"\n",
    "words = word_tokenize(quote.lower())\n",
    "words = remove_punctuation_and_stopwords(words)\n",
    "\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.2.\n",
    "\n",
    "What are the 25 most frequent words in *Brave New Worls*, if you disregard all the words that are on the `nltk` list of English stopwords? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said => 315\n",
      "one => 262\n",
      "bernard => 238\n",
      "savage => 201\n",
      "lenina => 195\n",
      "like => 181\n",
      "would => 116\n",
      "could => 116\n",
      "made => 112\n",
      "time => 112\n",
      "man => 111\n",
      "little => 111\n",
      "round => 108\n",
      "voice => 106\n",
      "eyes => 102\n",
      "linda => 101\n",
      "face => 100\n",
      "though => 97\n",
      "two => 97\n",
      "away => 94\n",
      "director => 93\n",
      "went => 92\n",
      "still => 91\n",
      "back => 91\n",
      "know => 90\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation_and_stopwords(words):\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        if w.isalnum() and w not in stopwords:\n",
    "            new_list.append( w )\n",
    "    return new_list\n",
    "\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "text_file = open('Corpus/BraveNewWorld.txt',encoding='utf-8')\n",
    "full_text = text_file.read()\n",
    "\n",
    "words = nltk.word_tokenize(full_text.lower())\n",
    "words = remove_punctuation_and_stopwords(words)\n",
    "\n",
    "freq = Counter(words)\n",
    "\n",
    "for word,count in freq.most_common(25):\n",
    "    print(f\"{word} => {count}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
