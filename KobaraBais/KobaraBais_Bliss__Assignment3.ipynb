{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5e9339d7",
      "metadata": {
        "id": "5e9339d7",
        "outputId": "3d9c3978-ec6e-4fde-bd32-d0451380862c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bliss_project_gutenberg.txt\n"
          ]
        }
      ],
      "source": [
        "from os.path import basename\n",
        "\n",
        "my_text = 'https://raw.githubusercontent.com/peterverhaar/dmt-2024/refs/heads/main/KobaraBais/Bliss_project_gutenberg.txt'\n",
        "file_name = basename(my_text)\n",
        "print(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f93899f6",
      "metadata": {
        "id": "f93899f6",
        "outputId": "1c2ed99a-278d-4122-cf19-cd018df7768f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (4.25.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.6)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from stanza) (2.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
            "Downloading stanza-1.9.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.14.0 stanza-1.9.2\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!pip install stanza\n",
        "!pip install vaderSentiment\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import requests\n",
        "from os.path import basename\n",
        "\n",
        "def download(url):\n",
        "    response = requests.get(url)\n",
        "    if response:\n",
        "        file_name = basename(url)\n",
        "        out = open(file_name,'w',encoding='utf-8')\n",
        "        out.write(response.text)\n",
        "        out.close()\n",
        "\n",
        "def sorted_by_value( dict , ascending = True ):\n",
        "    if ascending:\n",
        "        return {k: v for k, v in sorted(dict.items(), key=lambda item: item[1])}\n",
        "    else:\n",
        "        return {k: v for k, v in reversed( sorted(dict.items(), key=lambda item: item[1]))}\n",
        "\n",
        "download('https://raw.githubusercontent.com/peterverhaar/dmt-2024-assignment3/refs/heads/main/Assignment3.ipynb')\n",
        "download('https://raw.githubusercontent.com/peterverhaar/dmt-2024-assignment3/refs/heads/main/text_mining.py')\n",
        "download(my_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ab22f221",
      "metadata": {
        "id": "ab22f221"
      },
      "outputs": [],
      "source": [
        "from text_mining import *\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.text import Text\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import stanza\n",
        "import json\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1518b1b3",
      "metadata": {
        "id": "1518b1b3"
      },
      "outputs": [],
      "source": [
        "text_file = open(file_name,encoding='utf-8')\n",
        "full_text = text_file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "15f9409f",
      "metadata": {
        "id": "15f9409f",
        "outputId": "e37a2387-2b4e-4ca6-ce1a-1e748ba99cb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The short story  contains 4601 words\n"
          ]
        }
      ],
      "source": [
        "# Calculate number of words\n",
        "words = word_tokenize(full_text.lower())\n",
        "words = remove_punctuation(words)\n",
        "nr_tokens = len(words)\n",
        "\n",
        "print(f'The short story  contains {nr_tokens} words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fb9dbf37",
      "metadata": {
        "id": "fb9dbf37",
        "outputId": "dc3bdf2a-2516-4c32-92ad-c6277f66008e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The short story contains 1096 unique words\n"
          ]
        }
      ],
      "source": [
        "word_frequencies = Counter(words)\n",
        "print(f'The short story contains {len(word_frequencies)} unique words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "391f8112",
      "metadata": {
        "id": "391f8112",
        "outputId": "a284c005-da7a-4561-eea3-e31281ad16bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following 25 words are most common:\n",
            "the => 213\n",
            "and => 192\n",
            "she => 129\n",
            "a => 124\n",
            "to => 114\n",
            "her => 96\n",
            "it => 82\n",
            "of => 78\n",
            "in => 76\n",
            "was => 66\n",
            "i => 57\n",
            "that => 52\n",
            "you => 50\n",
            "with => 45\n",
            "bertha => 44\n",
            "had => 42\n",
            "said => 37\n",
            "t => 37\n",
            "so => 34\n",
            "he => 34\n",
            "as => 33\n",
            "they => 32\n",
            "up => 31\n",
            "s => 30\n",
            "what => 29\n"
          ]
        }
      ],
      "source": [
        "nr_words = 25\n",
        "\n",
        "print(f'The following {nr_words} words are most common:')\n",
        "\n",
        "for word,count in word_frequencies.most_common(nr_words):\n",
        "    print(f\"{word} => {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1ac1c09f",
      "metadata": {
        "id": "1ac1c09f",
        "outputId": "7d0cd208-faa5-4831-ec15-2a3db1377728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When we remove the stopwords, the following 25 words are most common:\n",
            "bertha 44\n",
            "said 37\n",
            "little 24\n",
            "harry 22\n",
            "fulton 22\n",
            "miss 20\n",
            "oh 18\n",
            "like 16\n",
            "come 11\n",
            "moment 11\n",
            "eddie 11\n",
            "face 11\n",
            "back 10\n",
            "yes 10\n",
            "seemed 10\n",
            "made 10\n",
            "one 10\n",
            "still 9\n",
            "must 9\n",
            "came 9\n",
            "fire 9\n",
            "something 8\n",
            "feeling 8\n",
            "way 8\n",
            "go 8\n",
            "silver 8\n",
            "baby 8\n",
            "put 8\n",
            "know 8\n",
            "turned 8\n"
          ]
        }
      ],
      "source": [
        "nr_words = 25\n",
        "\n",
        "words = remove_punctuation_and_stopwords(words)\n",
        "word_frequencies = Counter(words)\n",
        "\n",
        "print(f'When we remove the stopwords, the following {nr_words} words are most common:')\n",
        "\n",
        "for word,count in word_frequencies.most_common(30):\n",
        "    print(f\"{word} {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "849f990e",
      "metadata": {
        "id": "849f990e",
        "outputId": "355f9375-f041-41fc-9674-43e5dbe7a4d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The short story contains 346 sentences.\n",
            "The sentences contain 13.3 words on average\n"
          ]
        }
      ],
      "source": [
        "sentences = sent_tokenize(full_text)\n",
        "print(f'The short story contains {len(sentences)} sentences.')\n",
        "avg_nr_words = round((nr_tokens/len(sentences)),2)\n",
        "print(f'The sentences contain {avg_nr_words} words on average')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b25de79c",
      "metadata": {
        "id": "b25de79c",
        "outputId": "c8abfaa2-555b-4bd8-8b4f-413ff058c1ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 6 of 6 matches:\n",
            "                           bliss although bertha young thir\n",
            " overcome suddenly feeling bliss though suddenly swallowed \n",
            " shone transparent feeling bliss came back know express wan\n",
            " touch cool arm could fire bliss bertha know miss fulton lo\n",
            "d seemed fill brimming cup bliss still back mind pear tree \n",
            " ached ardent body feeling bliss leading dear said norman k\n"
          ]
        }
      ],
      "source": [
        "novel = Text(words)\n",
        "novel.concordance('bliss' , width = 60 , lines = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8d8377d2",
      "metadata": {
        "id": "8d8377d2",
        "outputId": "4b1c9b3c-c7af-45e3-da24-661ca1272876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it 4\n",
            "was 4\n",
            "oh 2\n",
            "yes 1\n",
            "what 2\n",
            "had 2\n",
            "she 4\n",
            "to 2\n",
            "say 1\n",
            "said 7\n",
            "voted 1\n",
            "her 2\n",
            "dullish 1\n",
            "and 13\n",
            "cold 1\n",
            "like 1\n",
            "all 1\n",
            "blond 1\n",
            "little 1\n",
            "on 2\n",
            "one 1\n",
            "side 1\n",
            "smiling 1\n",
            "has 2\n",
            "something 3\n",
            "behind 1\n",
            "i 4\n",
            "must 1\n",
            "find 1\n",
            "out 1\n",
            "that 2\n",
            "most 2\n",
            "likely 1\n",
            "s 4\n",
            "a 6\n",
            "good 1\n",
            "stomach 1\n",
            "answered 1\n",
            "were 1\n",
            "as 2\n",
            "much 1\n",
            "in 4\n",
            "love 1\n",
            "ever 1\n",
            "shouted 1\n",
            "hullo 1\n",
            "you 3\n",
            "people 1\n",
            "such 1\n",
            "zest 1\n",
            "for 2\n",
            "life 1\n",
            "wonder 1\n",
            "if 3\n",
            "miss 3\n",
            "fulton 4\n",
            "forgotten 1\n",
            "expect 1\n",
            "so 2\n",
            "ll 2\n",
            "run 1\n",
            "fat 1\n",
            "does 1\n",
            "coolly 1\n",
            "ringing 1\n",
            "the 6\n",
            "bell 1\n",
            "dinner 2\n",
            "face 2\n",
            "mug 1\n",
            "eddie 1\n",
            "their 3\n",
            "spoons 1\n",
            "rising 1\n",
            "lips 2\n",
            "with 2\n",
            "napkins 1\n",
            "enjoying 1\n",
            "his 3\n",
            "only 1\n",
            "have 2\n",
            "new 1\n",
            "coffee 2\n",
            "machine 1\n",
            "once 1\n",
            "fortnight 1\n",
            "light 1\n",
            "snapped 1\n",
            "made 1\n",
            "my 1\n",
            "dear 1\n",
            "knight 2\n",
            "don 2\n",
            "t 3\n",
            "ask 1\n",
            "me 1\n",
            "about 1\n",
            "sank 1\n",
            "into 1\n",
            "lowest 1\n",
            "deepest 1\n",
            "chair 1\n",
            "handed 1\n",
            "round 1\n",
            "cigarettes 1\n",
            "dislike 1\n",
            "whisky 1\n",
            "before 1\n",
            "go 1\n",
            "called 1\n",
            "moved 1\n",
            "towards 1\n",
            "hall 2\n",
            "bertha 1\n",
            "following 1\n",
            "when 1\n",
            "almost 1\n",
            "pushed 1\n",
            "past 1\n",
            "coat 1\n",
            "arms 1\n",
            "nostrils 1\n",
            "quivered 1\n",
            "curled 1\n",
            "back 1\n",
            "tomato 1\n",
            "soup 1\n",
            "is 1\n",
            "prefer 1\n",
            "voice 1\n",
            "very 1\n",
            "loud 1\n",
            "from 1\n",
            "can 1\n",
            "shut 1\n",
            "up 1\n",
            "shop 1\n",
            "extravagantly 1\n",
            "cool 1\n",
            "collected 1\n"
          ]
        }
      ],
      "source": [
        "freq = collocation( full_text , r'harry' , 20)\n",
        "\n",
        "for nearby_word in freq:\n",
        "    print(nearby_word , freq[nearby_word])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7adb77b1",
      "metadata": {
        "id": "7adb77b1"
      },
      "outputs": [],
      "source": [
        "adjective_codes = ['JJ','JJR','JJS']\n",
        "adverb_codes = ['RB','RBR','RBS']\n",
        "\n",
        "adjectives = Counter()\n",
        "adverbs = Counter()\n",
        "\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    words = remove_punctuation(words)\n",
        "\n",
        "    pos = pos_tag(words)\n",
        "\n",
        "    for p in pos:\n",
        "\n",
        "        if p[1] in adjective_codes:\n",
        "            adjectives.update([p[0]])\n",
        "\n",
        "        if p[1] in adverb_codes:\n",
        "            adverbs.update([p[0]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "194da397",
      "metadata": {
        "id": "194da397",
        "outputId": "68ce8cbd-149b-484f-ff1f-0a80a23d642a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('little', 20)\n",
            "('t', 9)\n",
            "('strange', 6)\n",
            "('white', 6)\n",
            "('last', 6)\n",
            "('good', 6)\n",
            "('cold', 5)\n",
            "('new', 5)\n",
            "('young', 5)\n",
            "('pear', 5)\n",
            "('other', 5)\n",
            "('bright', 4)\n",
            "('beautiful', 4)\n",
            "('right', 4)\n",
            "('dear', 4)\n",
            "('red', 4)\n",
            "('much', 4)\n",
            "('rare', 3)\n",
            "('big', 3)\n",
            "('dark', 3)\n",
            "('blue', 3)\n",
            "('yellow', 3)\n",
            "('Little', 3)\n",
            "('poor', 3)\n",
            "('black', 3)\n"
          ]
        }
      ],
      "source": [
        "for word in adjectives.most_common(25):\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "233efb4e",
      "metadata": {
        "id": "233efb4e",
        "outputId": "38597006-a26e-4a92-f8cc-96f1529a997a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('so', 34)\n",
            "('not', 28)\n",
            "('very', 11)\n",
            "('then', 10)\n",
            "('just', 10)\n",
            "('still', 9)\n",
            "('too', 9)\n",
            "('up', 8)\n",
            "('again', 8)\n",
            "('quite', 7)\n",
            "('really', 7)\n",
            "('back', 6)\n",
            "('simply', 5)\n",
            "('almost', 5)\n",
            "('yet', 5)\n",
            "('away', 5)\n",
            "('only', 5)\n",
            "('suddenly', 4)\n",
            "('most', 4)\n",
            "('Now', 4)\n",
            "('ever', 4)\n",
            "('lovely', 3)\n",
            "('rather', 3)\n",
            "('as', 3)\n",
            "('always', 3)\n"
          ]
        }
      ],
      "source": [
        "for word in adverbs.most_common(25):\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "79849b4b",
      "metadata": {
        "id": "79849b4b"
      },
      "outputs": [],
      "source": [
        "ana = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "fa87813b",
      "metadata": {
        "id": "fa87813b",
        "outputId": "64b45d39-db1e-415f-8fb8-2efc6bcb656c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive words:\n",
            "like\n",
            "yes\n",
            "feeling\n",
            "good\n",
            "bliss\n",
            "lovely\n",
            "loved\n",
            "dear\n",
            "laugh\n",
            "thank\n",
            "\n",
            "Negative words:\n",
            "miss\n",
            "no\n",
            "fire\n",
            "strange\n",
            "cried\n",
            "drunk\n",
            "poor\n",
            "_dreadful_\n",
            "idiotic\n",
            "forgotten\n"
          ]
        }
      ],
      "source": [
        "positive_words = []\n",
        "negative_words = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    for word in words:\n",
        "        scores = ana.polarity_scores(word)\n",
        "        if scores[\"pos\"] > 0.75:\n",
        "            positive_words.append(word)\n",
        "        elif scores[\"neg\"] > 0.75:\n",
        "            negative_words.append(word)\n",
        "\n",
        "print('Positive words:')\n",
        "\n",
        "sentiment_freq = Counter(positive_words)\n",
        "\n",
        "for word,count in sentiment_freq.most_common(10):\n",
        "    print(word)\n",
        "\n",
        "\n",
        "print('\\nNegative words:')\n",
        "\n",
        "sentiment_freq = Counter(negative_words)\n",
        "\n",
        "for word,count in sentiment_freq.most_common(10):\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "93c6e6fa",
      "metadata": {
        "id": "93c6e6fa",
        "outputId": "f50fe1a4-1ac1-4d44-e072-4e7ef31a7ac7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Postive sentences\n",
            "\n",
            "It was part of his—well, not his nature, exactly, and certainly not his pose—his—something or other—to talk about food and to glory in his “shameless passion for the white flesh of the lobster” and “the green of pistachio ices—green and cold like the eyelids of Egyptian dancers.” When he looked up at her and said: “Bertha, this is a very admirable _soufflée!_” she almost could have wept with child-like pleasure. [0.9481]\n",
            "Both, as it were, caught in that circle of unearthly light, understanding each other perfectly, creatures of another world, and wondering what they were to do in this one with all this blissful treasure that burned in their bosoms and dropped, in silver flowers, from their hair and hands? [0.9006]\n",
            "I like you.” And, indeed, she loved Little B so much—her neck as she bent forward, her exquisite toes as they shone transparent in the firelight—that all her feeling of bliss came back again, and again she didn’t know how to express it—what to do with it. [0.891]\n",
            "And friends—modern, thrilling friends, writers and painters and poets or people keen on social questions—just the kind of friends they wanted. [0.8887]\n",
            "As she was about to throw the last one she surprised herself by suddenly hugging it to her, passionately, passionately. [0.8885]\n",
            "Harry and she were as much in love as ever, and they got on together splendidly and were really good pals. [0.8883]\n",
            "And Mrs. Norman Knight: “Oh, Mr. Warren, what happy socks?” “I _am_ so glad you like them,” said he, staring at his feet. [0.8842]\n",
            "They were dears—dears—and she loved having them there, at her table, and giving them delicious food and wine. [0.875]\n",
            "They had met at the club and Bertha had fallen in love with her, as she always did fall in love with beautiful women who had something strange about them. [0.875]\n",
            "Bliss Although Bertha Young was thirty she still had moments like this when she wanted to run instead of walk, to take dancing steps on and off the pavement, to bowl a hoop, to throw something up in the air and catch it again, or to stand still and laugh at—nothing—at nothing, simply. [0.8689]\n",
            "\n",
            "Negative sentences\n",
            "\n",
            "“I have had such a _dreadful_ experience with a taxi-man; he was _most_ sinister. [-0.796]\n",
            "Absurd!” She sat up; but she felt quite dizzy, quite drunk. [-0.7915]\n",
            "“This is a sad, sad fall!” said Mug, pausing in front of Little B’s perambulator. [-0.7574]\n",
            "The fire had died down in the drawing-room to a red, flickering “nest of baby phœnixes,” said Face. [-0.7184]\n",
            "Tomato soup is so _dreadfully_ eternal.” “If you prefer,” said Harry’s voice, very loud, from the hall, “I can phone you a cab to come to the door.” “Oh, no. [-0.6948]\n",
            "What she and I have shared.” At those last words something strange and almost terrifying darted into Bertha’s mind. [-0.6378]\n",
            "But then then—— “My dear,” said Mrs. Norman Knight, “you know our shame. [-0.631]\n",
            "Oh, you should have seen her.” Bertha wanted to ask if it wasn’t rather dangerous to let her clutch at a strange dog’s ear. [-0.5994]\n",
            "How idiotic civilization is! [-0.5983]\n",
            "_Entendu_,” said Bertha, and hung up the receiver, thinking how more than idiotic civilization was. [-0.5965]\n"
          ]
        }
      ],
      "source": [
        "sent_scores = dict()\n",
        "\n",
        "sentences = [re.sub('\\n+', ' ' , sent) for sent in sentences]\n",
        "\n",
        "for s in sentences:\n",
        "    scores = ana.polarity_scores(s)\n",
        "    sent_scores[s] = scores['compound']\n",
        "\n",
        "nr_sentences = 10\n",
        "\n",
        "i = 0\n",
        "\n",
        "print('\\nPostive sentences\\n')\n",
        "\n",
        "for s in sorted_by_value( sent_scores , ascending = False ):\n",
        "    print( f'{s} [{ sent_scores[s]}]' )\n",
        "    i+= 1\n",
        "    if i == nr_sentences:\n",
        "        break\n",
        "\n",
        "print('\\nNegative sentences\\n')\n",
        "i = 0\n",
        "\n",
        "for s in sorted_by_value( sent_scores , ascending = True):\n",
        "    print( f'{s} [{ sent_scores[s]}]' )\n",
        "    i+= 1\n",
        "    if i == nr_sentences:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3d5a9f31",
      "metadata": {
        "id": "3d5a9f31"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def wordnet_hypernyms(token):\n",
        "    all_hypernyms = []\n",
        "    black_list = ['queen','young','human']\n",
        "\n",
        "    if token not in black_list:\n",
        "\n",
        "        word_senses = wn.synsets(token)\n",
        "        hypernyms = lambda s: s.hypernyms()\n",
        "        return_value = False\n",
        "        for ws in word_senses:\n",
        "\n",
        "            hypernyms = [hyp.name() for hyp in list(ws.closure(hypernyms))]\n",
        "            if 'plant.n.02' in hypernyms:\n",
        "                all_hypernyms.append('plant')\n",
        "            if 'color.n.01' in hypernyms:\n",
        "                all_hypernyms.append('colour')\n",
        "            if 'emotion.n.01' in hypernyms:\n",
        "                all_hypernyms.append('emotion')\n",
        "            if 'animal.n.01' in hypernyms:\n",
        "                all_hypernyms.append('animal')\n",
        "            if 'natural_phenomenon.n.01' in hypernyms:\n",
        "                all_hypernyms.append('natural_phenomenon')\n",
        "            if 'body_part.n.01' in hypernyms:\n",
        "                all_hypernyms.append('body_part')\n",
        "\n",
        "    return all_hypernyms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "029ba9d6",
      "metadata": {
        "id": "029ba9d6",
        "outputId": "73992994-7796-4920-c2cb-876b4e3d696a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common emotions:\n",
            "love\n",
            "fear\n",
            "quiver\n",
            "\n",
            "Most common colours:\n",
            "red\n",
            "blue\n",
            "yellow\n",
            "black\n",
            "purple\n",
            "green\n",
            "pink\n",
            "amber\n",
            "\n",
            "Most common body parts:\n",
            "face\n",
            "back\n",
            "lips\n",
            "head\n",
            "eyelids\n",
            "eye\n",
            "arm\n",
            "fingers\n",
            "ear\n",
            "neck\n",
            "stomach\n",
            "skins\n",
            "nose\n",
            "hand\n",
            "finger\n",
            "\n",
            "Most common natural phenomena:\n",
            "light\n",
            "glowing\n",
            "low\n",
            "winds\n",
            "drag\n",
            "moonlight\n",
            "pressure\n",
            "smoke\n",
            "lights\n",
            "moonbeam\n"
          ]
        }
      ],
      "source": [
        "word_list = []\n",
        "\n",
        "for word,count in word_frequencies.most_common():\n",
        "    hypernyms = wordnet_hypernyms(word)\n",
        "    if len(hypernyms)>0:\n",
        "        for h in hypernyms:\n",
        "            word_list.append((word,h))\n",
        "\n",
        "def find_most_common_hyponyms(domain):\n",
        "    common_hyponyms = []\n",
        "    for word, hyponym in word_list:\n",
        "        if hyponym == domain:\n",
        "            common_hyponyms.append(word)\n",
        "    return Counter(common_hyponyms)\n",
        "\n",
        "print(f\"Most common emotions:\")\n",
        "common_hyponyms = find_most_common_hyponyms('emotion')\n",
        "for word,count in common_hyponyms.most_common(15):\n",
        "    print(word)\n",
        "\n",
        "print(f\"\\nMost common colours:\")\n",
        "common_hyponyms = find_most_common_hyponyms('colour')\n",
        "for word,count in common_hyponyms.most_common(15):\n",
        "    print(word)\n",
        "\n",
        "print(f\"\\nMost common body parts:\")\n",
        "common_hyponyms = find_most_common_hyponyms('body_part')\n",
        "for word,count in common_hyponyms.most_common(15):\n",
        "    print(word)\n",
        "\n",
        "common_hyponyms = find_most_common_hyponyms('natural_phenomenon')\n",
        "if len(common_hyponyms)>0:\n",
        "    print(f\"\\nMost common natural phenomena:\")\n",
        "    for word,count in common_hyponyms.most_common(15):\n",
        "        print(word)\n",
        "\n",
        "\n",
        "common_hyponyms = find_most_common_hyponyms('amimal')\n",
        "if len(common_hyponyms)>0:\n",
        "    print(f\"\\nMost common animals:\")\n",
        "    for word,count in common_hyponyms.most_common(15):\n",
        "        print(word)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}