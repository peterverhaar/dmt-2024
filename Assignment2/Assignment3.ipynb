{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab22f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_mining import *\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import stanza\n",
    "import json\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7f95e",
   "metadata": {},
   "source": [
    "The novels are firstly downloaded from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1518b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open('1984.txt',encoding='utf-8')\n",
    "full_text = text_file.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15f9409f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel contains 101813 words\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of words\n",
    "words = word_tokenize(full_text.lower())\n",
    "words = remove_punctuation(words)\n",
    "nr_tokens = len(words)\n",
    "\n",
    "print(f'The novel contains {nr_tokens} words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9dbf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel contains 8600 unique words\n"
     ]
    }
   ],
   "source": [
    "word_frequencies = Counter(words)\n",
    "print(f'The novel contains {len(word_frequencies)} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "391f8112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following 20 words are most common:\n",
      "the 6473\n",
      "of 3485\n",
      "a 2573\n",
      "and 2417\n",
      "to 2333\n",
      "was 2317\n",
      "he 1965\n",
      "it 1862\n",
      "in 1855\n",
      "that 1479\n",
      "had 1346\n",
      "his 1086\n",
      "you 915\n",
      "not 855\n",
      "with 789\n",
      "as 721\n",
      "at 662\n",
      "for 661\n",
      "be 657\n",
      "they 638\n"
     ]
    }
   ],
   "source": [
    "nr_words = 20 \n",
    "\n",
    "print(f'The following {nr_words} words are most common:')\n",
    "\n",
    "for word,count in word_frequencies.most_common(nr_words):\n",
    "    print(f\"{word} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac1c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When we remove the stopwords, the following 20 words are most common:\n",
      "winston 522\n",
      "could 431\n",
      "one 426\n",
      "would 362\n",
      "said 341\n",
      "even 306\n",
      "party 289\n",
      "like 206\n",
      "time 199\n",
      "face 194\n",
      "thought 187\n",
      "seemed 173\n",
      "never 173\n",
      "two 163\n",
      "back 153\n",
      "moment 152\n",
      "still 150\n",
      "man 148\n",
      "always 145\n",
      "know 133\n",
      "way 127\n",
      "almost 127\n",
      "another 124\n",
      "words 123\n",
      "though 122\n",
      "might 122\n",
      "war 121\n",
      "eyes 120\n",
      "years 120\n",
      "little 114\n"
     ]
    }
   ],
   "source": [
    "nr_words = 20 \n",
    "\n",
    "words = remove_punctuation_and_stopwords(words)\n",
    "word_frequencies = Counter(words)\n",
    "\n",
    "print(f'When we remove the stopwords, the following {nr_words} words are most common:')\n",
    "\n",
    "for word,count in word_frequencies.most_common(30):\n",
    "    print(f\"{word} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "849f990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel contains 6696 sentences.\n",
      "The sentences contain 15.21 words on average\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(full_text)\n",
    "print(f'The novel contains {len(sentences)} sentences.')\n",
    "avg_nr_words = round((nr_tokens/len(sentences)),2) \n",
    "print(f'The sentences contain {avg_nr_words} words on average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25de79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
