{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the context of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances\n",
    "\n",
    "Lists of frequent words can be very useful: they can help to clarify the main concerns or the themes of a text. To examine *how* words are used in a text, it can be useful, additionally, to create a concordance. In a concordance, all the occurrences of a given search term are listed in combination with words that occur before and after this term. Such resources are sometimes referred to as *keyword in context* lists (KWIC). \n",
    "\n",
    "The `nltk` package contains a method named `concordance()`. To work with this method, you firstly need to create an instance of the `Text` class. This class is part of the `nltk.text` module. Such a `Text` object can be initialised using a list with all the tokens of a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "path = os.path.join('Corpus','PrideAndPrejudice.txt')\n",
    "\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "tokens = word_tokenize(full_text)\n",
    "novel = Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the `Text` object is given the name `novel`. \n",
    "\n",
    "Once you have created such an object, you can use its `concordance()` method. You can supply three parameters: \n",
    "\n",
    "1. A search term.\n",
    "2. A `width` parameter, specifying the extent of the context. With this parameter, you indicate the number of characters before and after the word whose context you want to see. \n",
    "3. A `lines` parameter, which specifies the number of results. \n",
    "\n",
    "Out of these parameters, only the first one is mandatory. When you leave out the last two parameters, the method will work with its default values: a width of 70 (35 characters before and 35 characters after the search term) and 25 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 66 matches:\n",
      "month . Happiness in marriage is entirely a matter\n",
      "ng of their supposed marriage , and planning his h\n",
      " well disposed of in marriage . This gallantry was\n",
      "probability of their marriage was exceedingly agre\n",
      "the felicity which a marriage of true affection co\n",
      "hat another offer of marriage may ever be made you\n",
      "for happiness in the marriage state . If therefore\n",
      "made you an offer of marriage . Is it true ? '' El\n",
      "llâ€”and this offer of marriage you have refused ? '\n",
      "using every offer of marriage in this way , you wi\n"
     ]
    }
   ],
   "source": [
    "novel.concordance('marriage' , width = 50 , lines = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `concordance()` method that is defined in `nltk`, the width of the context is defined using a specific number of characters. When you work with such a fixed number of characters, the search term can be shown at the same position on each line, resulting in a keyword-in-context list with a nice and orderly appearance.\n",
    "\n",
    "The downside of this approach is that the various lines may contain incomplete words. Which you indicate that the size of the context must be set at 20 characters before and after the term, the code simply removes all characters preceding or following the twentieth character. \n",
    "\n",
    "The cell below contains a definition of a method which can create a somwhat different type of concordance. In this method, named `concordance_words()`, the width of the context is specified on the basis of words rather characters. When you supply the number 10 as the value for the parameter defining the width, you will receive all occurrences of the search term, together with 5 words before and 5 words after this search term. The method demands a sting as input. This string can be the full text of a novel. \n",
    "\n",
    "The results are returned as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from text_mining import *\n",
    "\n",
    "def concordance_word( text, regex , width = 10 ):\n",
    "\n",
    "    concordance = []\n",
    "    distance = math.floor( width /2 )\n",
    "\n",
    "    segment_length = 0\n",
    "\n",
    "    words = word_tokenize( text )\n",
    "    words = remove_punctuation( words )\n",
    "    i = 0\n",
    "    for w in words:\n",
    "        if re.search( regex , w , re.IGNORECASE ):\n",
    "            match = ''\n",
    "            for x in range( i - distance , ( i + distance ) + 1 ):\n",
    "                if x >= 0 and x < len(words):\n",
    "                    if len(words[x]) >= 0:\n",
    "                        match += words[x] + ' '\n",
    "            concordance.append( match )\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return concordance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains an illustration of how you can use this method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 67 ocurrences of the word \"marriage\".\n",
      "Here are the first 5 occurrences:\n",
      "\n",
      "\n",
      "studying his character for a twelvemonth Happiness in marriage is entirely a matter of chance If the \n",
      "\n",
      "disliking her guest by talking of their supposed marriage and planning his happiness in such an alliance \n",
      "\n",
      "all in due time well disposed of in marriage This gallantry was not much to the taste \n",
      "\n",
      "her to understand that the probability of their marriage was exceedingly agreeable to her Elizabeth however did \n",
      "\n",
      "very house in all the felicity which a marriage of true affection could bestow and she felt \n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join('Corpus','PrideAndPrejudice.txt')\n",
    "\n",
    "with open( path, encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "    \n",
    "fragments = concordance_word( full_text , r'marriage' , 16)\n",
    "\n",
    "print( f'There are {len(fragments)} ocurrences of the word \"marriage\".')\n",
    "\n",
    "number_of_results = 5\n",
    "\n",
    "print( f'Here are the first {number_of_results} occurrences:\\n\\n')\n",
    "for f in fragments[:number_of_results]:\n",
    "    print( f'{f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the definition of `concordance_word()`, the method searches for occurrences of the supplied search term as a [regular expression](https://cdsleiden.github.io/python-tutorial/notebooks/9%20Regular_expressions.html). The second parameter of this method can also be a more complicated regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best education can overcome And your defect is a propensity to hate every body And yours he replied with a smile is wilfully to \n",
      "\n",
      "there is not a bit of fish to be got Lydia my love ring the bell I must speak to Hill this moment It is \n",
      "\n",
      "of him to write to you at all and very hypocritical I hate such false friends Why could not he keep on quarrelling with you \n",
      "\n",
      "is that we are very different sort of men and that he hates me This is quite shocking deserves to be publicly disgraced Some time \n",
      "\n",
      "misfortune of all find a man agreeable whom one is determined to hate not wish me such an evil When the dancing recommenced however and \n",
      "\n",
      "I shall chuse to attribute it to your wish of increasing my love by suspense according to the usual practice of elegant females I do \n",
      "\n",
      "Collins was not left long to the silent contemplation of his successful love for Bennet having dawdled about in the vestibule to watch for the \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fragments = concordance_word(full_text , r'(\\bhates?\\b)|(\\bloves?\\b)' , 25)\n",
    "\n",
    "for f in fragments[15:22]:\n",
    "    print( f'{f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation analysis\n",
    "\n",
    "Collocation analyses focus on the words that are used in the vicinity of a provided search term. It may be viewed as an extension of the principle underlying the creation of concordances. To perform a collocation analysis, we can look at the environments of a search term through a 'window' consisting of a given number of words. The words that are used in this context can obviously be counted. The aim of a collocation analysis is to identify the words that are used most frequently in the neighbourhood of a given word. \n",
    "\n",
    "Such collocation analyses can be carried out using the `collocation()` method that is defined below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation( text , regex , width ):\n",
    "\n",
    "    freq_c = dict()\n",
    "    distance = math.floor(width/2)\n",
    "\n",
    "    sentences = sent_tokenize( text )\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        words = word_tokenize( sentence )\n",
    "        words = remove_punctuation(words)\n",
    "\n",
    "        for i,w in enumerate(words):\n",
    "            if re.search( regex , w , re.IGNORECASE ):\n",
    "                index_regex = i \n",
    "\n",
    "                for x in range( i - distance , i + distance ):\n",
    "                    if x >= 0 and x < len(words) and words[x].lower() != words[index_regex].lower():\n",
    "                        if len(words[x]) > 0:\n",
    "                            word = words[x].lower()\n",
    "                            freq_c[ word ] = freq_c.get( word , 0 ) + 1\n",
    "            \n",
    "    return freq_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are the same as those of the `concordance_word()` method: \n",
    "\n",
    "1. The text that needs to be analysed.\n",
    "2. A search term, which will be treated as a regular expression.\n",
    "3. A number representing the width of the context (or, ot be more precise: the number of words). \n",
    "\n",
    "This function returns a dictionary listing all the words found near the search term that is provided, together with the frequencies of these words.  \n",
    "\n",
    "The code below makes use of the function `sortedByValue()` which can sort a dictionary by value, and the list of stopwords from `nltk` to remove the function words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_words = collocation( full_text , r'marriage' , 20)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "nearby_words_sorted = sortedByValue( nearby_words , ascending = False)\n",
    "\n",
    "for word in list( nearby_words_sorted.keys() ):\n",
    "    freq = nearby_words_sorted[word]\n",
    "    if word not in stopwords and freq > 2:\n",
    "        print( f'{word} => {freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooccurrence\n",
    "\n",
    "Once you have established that two specific words are often used in combination, you can begin to study specific combinations of words in more detail using the `cooccurrence()` method that is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence( text , word1 , word2 , width ):\n",
    "    \n",
    "    relevant_sentences = []\n",
    "    \n",
    "    text = re.sub( '\\s+' , ' ' , text )\n",
    "    sentences = sent_tokenize( text )\n",
    "\n",
    "    for s in sentences:\n",
    "        if re.search( r'\\b' + word1 + r'\\b' , s , re.IGNORECASE ) and re.search( r'\\b' + word2 + r'\\b' , s , re.IGNORECASE ):\n",
    "\n",
    "            words = word_tokenize(s)\n",
    "            word1_indexes = []\n",
    "            word2_indexes = []\n",
    "            \n",
    "            for i,w in enumerate(words):\n",
    "                if re.search( r'\\b' + word1 + r'\\b' , w , re.IGNORECASE ):\n",
    "                    word1_indexes.append(i)\n",
    "                elif re.search( r'\\b' + word2 + r'\\b' , w , re.IGNORECASE ):\n",
    "                    word2_indexes.append(i)\n",
    "\n",
    "            if word1_indexes[0] > word2_indexes[0]:\n",
    "                difference = word1_indexes[0] - word2_indexes[0]\n",
    "            else:\n",
    "                difference = word2_indexes[0] - word1_indexes[0]\n",
    "\n",
    "            if difference <= width:\n",
    "                relevant_sentences.append(s)\n",
    "    return relevant_sentences\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The useage of the method is as follows:\n",
    "    \n",
    "* As the first parameter, you mus provide the full text that you want to analyse, as a single string.\n",
    "* As the second and the third parameter, you need to mention the two words that you are interested in. \n",
    "* How close should these two words be? The fourth parameter specifies the number of words that are allowed in between the two words you focus on.  \n",
    "\n",
    "The method `cooccurrence()` returns all the sentences containing the two words that you focus on. The distance (measured in number of words) will not be greater than the width that you specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = cooccurrence( full_text , 'marriage' , 'lydia' , 10 )\n",
    "\n",
    "for s in sentences:\n",
    "    print( f'{s}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.1\n",
    "\n",
    "Create a concordance for the word 'savage' in the novel *Brave New World*. You can find the full text in the 'Corpus' folder. Work with a width of 50 characters (i.e. 25 characters before and 25 characters after this search term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 201 matches:\n",
      "d me to go to one of the Savage Reservations with him . \n",
      "e always wanted to see a Savage Reservation . ' 'But his\n",
      "cause I do want to see a Savage Reservation . ' * * * * \n",
      "eek they would be in the Savage Reservation . Not more t\n",
      "e had ever been inside a Savage Reservation . As an Alph\n",
      "here is no escape from a Savage Reservation . ' The word\n",
      "nted to the sullen young savage . 'Funny , I expect . ' \n",
      "nce more the men 's deep savage affirmation of their man\n",
      "e cheek . 'Turned into a savage , ' she shouted . 'Havin\n",
      " 'father ' of this young savage must be . 'Would you lik\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.text import Text\n",
    "import os\n",
    "\n",
    "path = os.path.join('Corpus','BraveNewWorld.txt')\n",
    "\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "tokens = word_tokenize(full_text)\n",
    "novel = Text(tokens)\n",
    "\n",
    "novel.concordance('savage' , width = 56, lines = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.2. \n",
    "\n",
    "Create a concordance for the word 'soma' in the novel *Brave New World*. This time, work with a width of 20 words (i.e. 10 words before and 10 words after this search term). Display the first 15 occurrences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 61 ocurrences of the word \"soma\".\n",
      "Here are the first 15 occurrences:\n",
      "\n",
      "\n",
      "that brute Henry Foster you need is a gramme of soma the advantages of Christianity and alcohol none of their defects \n",
      "\n",
      "in the solid substance of their distractions there is always soma delicious soma half a gramme for a a gramme for \n",
      "\n",
      "solid substance of their distractions there is always soma delicious soma half a gramme for a a gramme for a two \n",
      "\n",
      "that he could have got through life without ever touching soma The malice and bad tempers from which other people had \n",
      "\n",
      "do look glum What you need is a gramme of soma Diving into his Benito produced a phial cubic centimetre cures \n",
      "\n",
      "be true his brain I suppose He put away the soma bottle and taking out a packet of stuffed a plug \n",
      "\n",
      "a loud and cheerful company they ate an excellent meal Soma was served with the coffee Lenina took two tablets and \n",
      "\n",
      "half an hour before closing time that second dose of soma had raised a quite impenetrable wall between the actual universe \n",
      "\n",
      "she was and in spite of that second gramme of soma Lenina did not forget to take all the contraceptive precautions \n",
      "\n",
      "T and sat down The service had begun The dedicated soma tablets were placed in the centre of the dinner table \n",
      "\n",
      "centre of the dinner table The loving cup of strawberry soma was passed from hand to hand and with the formula \n",
      "\n",
      "has but begun Again twelve stanzas By this time the soma had begun to work Eyes shone cheeks were flushed the \n",
      "\n",
      "to Lenina friends of whom they met dozens in the soma bar between the wrestling bouts and in spite of his \n",
      "\n",
      "of all she continued in another tone you do take soma when you have these dreadful ideas of yours You forget \n",
      "\n",
      "were back in his rooms Bernard swallowed four tablets of soma at a gulp turned on the radio and television and \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import os\n",
    "\n",
    "from text_mining import *\n",
    "\n",
    "path = os.path.join('Corpus','BraveNewWorld.txt')\n",
    "\n",
    "with open( path, encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "    \n",
    "fragments = concordance_word( full_text , r'soma' , 20)\n",
    "\n",
    "print( f'There are {len(fragments)} ocurrences of the word \"soma\".')\n",
    "\n",
    "number_of_results = 15\n",
    "\n",
    "print( f'Here are the first {number_of_results} occurrences:\\n\\n')\n",
    "for f in fragments[:number_of_results]:\n",
    "    print( f'{f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.3\n",
    "\n",
    "In *Ullyses*, which words are used most frequently in the vicinity of the word 'father'? Consider 8 words before and 8 words after all the occurrences of this specific name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conmee => 62\n",
      "cowley => 28\n",
      "son => 27\n",
      "said => 18\n",
      "conroy => 11\n",
      "left => 11\n",
      "like => 9\n",
      "house => 9\n",
      "old => 9\n",
      "mother => 9\n",
      "simon => 8\n",
      "john => 8\n",
      "saluted => 8\n",
      "mr => 8\n",
      "walked => 8\n",
      "reverend => 7\n",
      "little => 7\n",
      "thought => 7\n",
      "stephen => 7\n",
      "man => 7\n",
      "time => 7\n",
      "put => 7\n",
      "handed => 6\n",
      "dedalus => 6\n",
      "smiled => 6\n",
      "blessed => 6\n",
      "told => 6\n",
      "father => 6\n",
      "poor => 6\n",
      "bernard => 6\n",
      "thy => 6\n",
      "ghost => 6\n",
      "hamlet => 6\n",
      "go => 5\n",
      "hanlon => 5\n",
      "malachi => 5\n",
      "along => 5\n",
      "road => 5\n",
      "letter => 5\n",
      "flynn => 5\n",
      "name => 5\n",
      "well => 5\n",
      "friend => 5\n",
      "holy => 5\n",
      "knows => 5\n",
      "god => 5\n",
      "gave => 5\n",
      "captain => 4\n",
      "bloom => 4\n",
      "canon => 4\n",
      "hughes => 4\n",
      "forget => 4\n",
      "virag => 4\n",
      "black => 4\n",
      "constable => 4\n",
      "passed => 4\n",
      "corner => 4\n",
      "hat => 4\n",
      "dineen => 4\n",
      "race => 4\n",
      "sir => 4\n",
      "going => 4\n",
      "art => 4\n",
      "would => 4\n",
      "eyes => 4\n",
      "things => 4\n",
      "child => 4\n",
      "first => 4\n",
      "looked => 4\n",
      "laid => 4\n",
      "voice => 4\n",
      "ennis => 3\n",
      "waiting => 3\n",
      "theyre => 3\n",
      "always => 3\n",
      "used => 3\n",
      "bit => 3\n",
      "corrigan => 3\n",
      "church => 3\n",
      "years => 3\n",
      "must => 3\n",
      "respected => 3\n",
      "dolan => 3\n",
      "thinking => 3\n",
      "air => 3\n",
      "dublin => 3\n",
      "saint => 3\n",
      "hope => 3\n",
      "good => 3\n",
      "talking => 3\n",
      "ancient => 3\n",
      "patrick => 3\n",
      "back => 3\n",
      "cried => 3\n",
      "famous => 3\n",
      "ross => 3\n",
      "dilly => 3\n",
      "read => 3\n",
      "help => 3\n",
      "conductor => 3\n",
      "seemed => 3\n",
      "head => 3\n",
      "saw => 3\n",
      "went => 3\n",
      "right => 3\n",
      "north => 3\n",
      "turned => 3\n",
      "street => 3\n",
      "great => 3\n",
      "red => 3\n",
      "bright => 3\n",
      "provincial => 3\n",
      "walking => 3\n",
      "indeed => 3\n",
      "unborn => 3\n",
      "grandfather => 3\n",
      "envy => 3\n",
      "youth => 3\n",
      "decline => 3\n",
      "new => 3\n",
      "wrote => 3\n",
      "says => 3\n",
      "wife => 3\n",
      "forty => 3\n",
      "call => 3\n",
      "might => 3\n",
      "queen => 3\n",
      "one => 3\n",
      "could => 3\n",
      "day => 3\n",
      "walk => 3\n",
      "got => 3\n",
      "coffey => 3\n",
      "dead => 3\n",
      "quay => 3\n",
      "wise => 3\n",
      "floating => 3\n",
      "vaughan => 3\n",
      "hear => 3\n",
      "consubstantial => 3\n",
      "jew => 3\n",
      "shakespeare => 3\n",
      "grandson => 3\n"
     ]
    }
   ],
   "source": [
    "from text_mining import *\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def sortedByValue( dict , ascending = True ):\n",
    "    if ascending: \n",
    "        return {k: v for k, v in sorted(dict.items(), key=lambda item: item[1])}\n",
    "    else:\n",
    "        return {k: v for k, v in reversed( sorted(dict.items(), key=lambda item: item[1]))}\n",
    "\n",
    "path = os.path.join('Corpus','Ullyses.txt')\n",
    "\n",
    "with open( path, encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "    \n",
    "nearby_words = collocation( full_text , r'father' , 20)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "nearby_words_sorted = sortedByValue( nearby_words , ascending = False)\n",
    "\n",
    "for word in list( nearby_words_sorted.keys() ):\n",
    "    freq = nearby_words_sorted[word]\n",
    "    if word not in stopwords and freq > 2:\n",
    "        print( f'{word} => {freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.4\n",
    "\n",
    "Find all the sentences in *Ullyses* that contain the words 'book' and 'read'. Make sure that, in these sentences, there are no more than 10 words in between these two words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_mining import *\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "path = os.path.join('Corpus','Ullyses.txt')\n",
    "\n",
    "with open( path, encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "    \n",
    "cooccurrences = cooccurrence( full_text , 'book' , 'read' , 10 )\n",
    "\n",
    "for fragment in cooccurrences:\n",
    "    print(fragment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
