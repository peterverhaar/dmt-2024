{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Quantitative analyses of texts are often based initially on counts of the smaller linguistic units that occur within texts, such as the words, the paragraph, or the sentences. The process of dividing a text into such smaller components is referred to as tokenisation. \n",
    "\n",
    "Word tokenisation generally takes place on the basis of the spaces that occur in between the words. The individual words that are found in a text are called “**tokens**\". When this full list is deduplicated, leaving only the unique words, the items in such lists are called “**types**”. \n",
    "\n",
    "This tutorial explains how you can tokenise a text using the `nltk` package, a toolkit that enables you you work with texts in natural languages. \n",
    "\n",
    "To make use of this package, you firstly need to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1250429073.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\s2929392\\AppData\\Local\\Temp\\ipykernel_9072\\1250429073.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    import nltk /enter\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods that are discussed in this tutorial make use of a number of additional resources which are not installed by default. If you have never used `nltk` before, you need to run the code below to install these resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s2929392\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\s2929392\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\s2929392\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s2929392\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\s2929392\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\sentiwordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenisation \n",
    "\n",
    "The `ntlk.tokenize` module has a method named `word_tokenize()`. This methods demand a string as input. This may be a sentence or a whole paragraph. When it is run, the method returns a Python list containing all the individual words found in the string that is provided. As mentioned, the individual words are identified using the spaces found in this string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times', ',', 'it', 'was', 'the', 'worst', 'of', 'times']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "quote = '''\n",
    "It was the best of times, \n",
    "it was the worst of times'''\n",
    "\n",
    "words = word_tokenize(quote)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look closely at the output of the code above, you can see that the `word_tokenize()` method treats punctuation marks as separate tokens. In the example above, the comma following the first occurrence of the word 'times' is actually a separate item in the list. \n",
    "\n",
    "The function `remove_punctuation`, defined below, can be used to remove the tokens that consist of punctuation only. As input, the function demands a list of tokens. For each string in this list, the function tests whether it consists of alphanumeric characters, using the `isalnum()` method. The function returns those words which pass this test only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        if w.isalnum():\n",
    "            new_list.append( w )\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']\n"
     ]
    }
   ],
   "source": [
    "words = remove_punctuation(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created a list containing all the individual tokens, you can easily count the total number of token, using the `len()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text fragment contains 12 tokens.\n"
     ]
    }
   ],
   "source": [
    "nr_tokens = len( words )\n",
    "\n",
    "print( f'The text fragment contains {nr_tokens} tokens.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tokenisation\n",
    "\n",
    "The method `sent_tokenize()` from the `nltk` package can be used to divide a text into its separate sentences. This type of tokenisation take place on the basis of full stops and upper case characters. \n",
    "\n",
    "The cell below contains an illustration of how the `sent_tokenize()` method can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fragment contains 4 sentences.\n",
      "\n",
      "In the late summer of that year we lived in a house in a village that looked across the river and the plain to the mountains.\n",
      "\n",
      "In the bed of the river there were pebbles and boulders, dry and white in the sun, and the water was clear and swiftly moving and blue in the channels.\n",
      "\n",
      "Troops went by the house and down the road and the dust they raised powdered the leaves of the trees.\n",
      "\n",
      "The trunks of the trees too were dusty and the leaves fell early that year and we saw the troops marching along the road and the dust rising and leaves, stirred by the breeze, falling and the soldiers marching and afterward the road bare and white except for the leaves.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "## First paragraph of \"A Farewell to Arms\"\n",
    "quote = '''In the late summer of that year we lived in a house in a village that looked across the river and the plain to the mountains. In the bed of the river there were pebbles and boulders, dry and white in the sun, and the water was clear and swiftly moving and blue in the channels. Troops went by the house and down the road and the dust they raised powdered the leaves of the trees. The trunks of the trees too were dusty and the leaves fell early that year and we saw the troops marching along the road and the dust rising and leaves, stirred by the breeze, falling and the soldiers marching and afterward the road bare and white except for the leaves.'''\n",
    "\n",
    "sentences = sent_tokenize(quote)\n",
    "\n",
    "print( f'The fragment contains { len(sentences) } sentences.\\n' )\n",
    "\n",
    "for s in sentences:\n",
    "    print(s + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1.\n",
    "\n",
    "The 'Corpus' that you have downloaded as part of this tutorial contains the full text of 'Pride and Prejudice'. \n",
    "Write code that can help you to answer the following questions about this text:\n",
    "\n",
    "* How many characters are there in the novel?\n",
    "* How many words does the novel contain?\n",
    "* How often does the novel mention the word \"marriage\"?\n",
    "* How long are the words in *Pride and Prejudice* on average (measured in number of characters)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11132\\1151114889.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfull_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "text_file = open ('corpus/PrideandPrejudice.txt', encoding ='utf-8')\n",
    "full_text = text_file.read()\n",
    "\n",
    "words = nltk.word_tokenize(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143569\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(full_text)\n",
    "\n",
    "words.count ('marriage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120081\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "print(len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to => 4076\n",
      "the => 4056\n",
      "of => 3605\n",
      "and => 3371\n",
      "her => 2131\n",
      "I => 2020\n",
      "a => 1874\n",
      "was => 1842\n",
      "in => 1783\n",
      "that => 1520\n",
      "not => 1506\n",
      "she => 1378\n",
      "it => 1273\n",
      "be => 1230\n",
      "his => 1191\n",
      "had => 1148\n",
      "you => 1137\n",
      "as => 1132\n",
      "he => 1096\n",
      "for => 1029\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "frequencies = Counter(words)\n",
    "\n",
    "for word,count in frequencies.most_common(20):\n",
    "    print(f\"{word} => {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2.\n",
    "\n",
    "Can you print the first five sentences of *Pride and Prejudice*?\n",
    "How many sentences does the novel contain in total?\n",
    "Building on the results of the previous exercise, can you calculate the average length of sentences? In other words, how many tokens do the sentences contain on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.3.\n",
    "\n",
    "What is the longest word in the novel *Ullyses*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open ('corpus/Ullyses.txt', encoding ='utf-8')\n",
    "full_text = text_file.read()\n",
    "\n",
    "print(len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
