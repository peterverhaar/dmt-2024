{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging and Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Natural Language Processing is an interdisciplinary area of research bringing together insights from fields such as artifical intelligence, computational linguistics, statistics and computer science. The aim of NLP is to enable computers to understand and to process the natural langauges that are spoken and written by human beings. Researchers in the field of NLP have developed sophisticated tools and algorithms for machine translation, document summarisation and sentiment analysis. This notebook explains two specific preprocessing task in NLP: Part of speech tagging and Lemmatisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging\n",
    "\n",
    "Part of speech (POS) taggers are applications which can produce data about the syntactic categories of words. They can create labels such as 'noun', 'adjecvive', 'adverb' or 'determiner' for the words in a text. Using such POS taggers, we can also extract words in specific syntactic categories.\n",
    "\n",
    "Once you have imported the `nltk` library, you can generate such POS tags by making use of the `pos_tag()` method. This method demands a list of words as a parameter. \n",
    "\n",
    "`pos_tag()` is typically used in combination with a word tokenisation method such as `word_tokenize`. The output of this latter function can then be used as input to the `pos_tag()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All => PDT\n",
      "the => DT\n",
      "world => NN\n",
      "is => VBZ\n",
      "a => DT\n",
      "stage => NN\n",
      "and => CC\n",
      "all => PDT\n",
      "the => DT\n",
      "men => NNS\n",
      "and => CC\n",
      "women => NNS\n",
      "merely => RB\n",
      "players => NNS\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "\n",
    "quote = '''All the world is a stage, \n",
    "and all the men and women merely players'''\n",
    "\n",
    "words = word_tokenize(quote)\n",
    "words = remove_punctuation(words)\n",
    "pos = pos_tag(words)\n",
    "\n",
    "for p in pos:\n",
    "    print(p[0] + ' => ' + p[1] )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pos_tag()` methods returns a composite variable with two values. More specifically, it is a data structure that is called a *tuple*. The first value is the word that was tagged and the second value is the POS tag that was assigned to this word. You can access these values individually using square brackets. \n",
    "\n",
    "The meaning of all of the POS tags can be displayed by printing the output of the `nltk.help.upenn_tagset()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print( nltk.help.upenn_tagset() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of codes and their meanings can [also be found online](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation \n",
    "\n",
    "English verbs can be used in the past tense, in the present tense, in the continuous form or in the perfect form, among other forms. These different tenses and forms can evidently make it more difficult to search systematically for occurrences of a specific verb. The same can be the case for nouns. They can be used in the singular and in the plural form. In some situations, we simply want to find all occurrences of a word, regardless of declensions and inflections. In this context, lemmatisation can offer a solution.\n",
    "\n",
    "Lemmatisation is a process in which the conjugated forms of the words that are found in a text are converted into their base dictionary form. This base form is referred to as the lemma. \n",
    "\n",
    "You can lemmatise texts using the  `lemmatize()` method, which is part of the `WordNetLemmatizer` module of the `nltk` library. This method needs to be applied to indivual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n",
      "read\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "print( lemmatiser.lemmatize( 'books' ) )\n",
    "## prints 'book'\n",
    "\n",
    "print( lemmatiser.lemmatize( 'reads' ) )\n",
    "## prints 'read'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It some cases, it can be unclear precisely how words ought to be lemmatised. Certain homonyms may either be verbs or nouns, for instance, and, depending on their usage, they should be lemmatised to different forms. To help the lemmatiser to make such distinctions, we can add a second parameter to indicate the lexical category of the word to be lemmatised. The first statement in the code lemmatises the word 'recording' as a verb, and the second statement as a noun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record\n",
      "recording\n"
     ]
    }
   ],
   "source": [
    "print( lemmatiser.lemmatize( 'recording' , 'v') )\n",
    "## 'record'\n",
    "\n",
    "print( lemmatiser.lemmatize( 'recording' , 'n' ) )\n",
    "## 'recording'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `lemmatize()` method does not use the Penn Treebank codes but the POS codes that have been defined for `wordnet`. It uses 'a' for adjectives, 'v' for verbs, 'n for nouns' and  'r' for adverbs. \n",
    "\n",
    "The code below shows you how you can lemmatise a whole sentence. The code firstly tokenises the words in the sentence that is given using `word_tokenize`. Next, the code generates the POS codes (from Penn Treebank) using `pos_tag`. These codes are then converted into `wordnet` codes used a new function named `ptb_to_wordnet()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we => we\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\S2929392/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\S2929392\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\S2929392/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\S2929392\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1208\\2345484870.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34mr'\\w+'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mposTag\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mlemma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mposTag\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34mf'{word} => {lemma}'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \"\"\"\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__reader_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m# This is where the magic happens!  Transform ourselves into\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1174\u001b[0m             )\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprovenances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0momw_prov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# A cache to store the wordnet data of multiple languages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36momw_prov\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[0mprovdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0mprovdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eng\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_omw_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[0mprov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlangfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\S2929392/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\S2929392\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "quote = '''We are such stuff as dreams are made on, \n",
    "and our little life is rounded with a sleep'''\n",
    "\n",
    "\n",
    "def ptb_to_wordnet(PTT):\n",
    "\n",
    "    if PTT.startswith('J'):\n",
    "        ## Adjective\n",
    "        return 'a'\n",
    "    elif PTT.startswith('V'):\n",
    "        ## Verb\n",
    "        return 'v'\n",
    "    elif PTT.startswith('N'):\n",
    "        ## Noune\n",
    "        return 'n'\n",
    "    elif PTT.startswith('R'):\n",
    "        ## Adverb\n",
    "        return 'r'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "    \n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "words = word_tokenize(quote)\n",
    "words = remove_punctuation(words)\n",
    "\n",
    "pos = nltk.pos_tag(words)\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    word = word.lower()\n",
    "    posTag = ptb_to_wordnet( pos[i][1] )\n",
    "    \n",
    "    if re.search( r'\\w+' , posTag , re.IGNORECASE ):\n",
    "        lemma = lemmatiser.lemmatize( words[i] , posTag )\n",
    "        print( f'{word} => {lemma}' )\n",
    "    else:\n",
    "        print( f'{word} => {word}' )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.1\n",
    "\n",
    "Create a list containing the unique adjectives that are occur in *Pride and Prejudice*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "such => 285\n",
      "other => 208\n",
      "much => 204\n",
      "little => 179\n",
      "own => 179\n",
      "good => 172\n",
      "more => 155\n",
      "great => 141\n",
      "young => 126\n",
      "last => 118\n",
      "many => 115\n",
      "first => 109\n",
      "dear => 95\n",
      "sure => 92\n",
      "happy => 80\n",
      "few => 71\n",
      "same => 68\n",
      "next => 67\n",
      "least => 60\n",
      "better => 59\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "\n",
    "path = os.path.join('Corpus','PrideAndPrejudice.txt')\n",
    "\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "words = word_tokenize(full_text)\n",
    "words = remove_punctuation(words)\n",
    "pos = pos_tag(words)\n",
    "\n",
    "adjectives = []\n",
    "adj_codes = ['JJ','JJR','JJS']\n",
    "\n",
    "for p in pos:\n",
    "    if p[1] in adj_codes:\n",
    "        adjectives.append(p[0])\n",
    "        \n",
    "freq = Counter(adjectives)\n",
    "\n",
    "for word,count in freq.most_common(20):\n",
    "    print(f'{word} => {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.2\n",
    "\n",
    "Stephen King is [reputed to have said](https://www.goodreads.com/quotes/430289-i-believe-the-road-to-hell-is-paved-with-adverbs) that “the road to hell is paved with adverbs\", and many style guides similarly give writers the advice to avoid adverbs, especially those ending in '-ly'. \n",
    "\n",
    "Can you calculate, for each text in the corpus, the number of adverb ending in '-ly', measured as a percentage of the total number of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BraveNewWorld.txt\n",
      "1203 adverbs ending in '-ly' in total.\n",
      "This is 0.0195% of all the words \n",
      "15 most frequent adverbs:\n",
      "only => 89\n",
      "suddenly => 81\n",
      "really => 50\n",
      "slowly => 30\n",
      "actually => 21\n",
      "simply => 13\n",
      "absolutely => 11\n",
      "simultaneously => 10\n",
      "merely => 10\n",
      "beastly => 10\n",
      "finally => 10\n",
      "carefully => 9\n",
      "particularly => 9\n",
      "hardly => 9\n",
      "perfectly => 8\n",
      "\n",
      "PrideandPrejudice.txt\n",
      "1891 adverbs ending in '-ly' in total.\n",
      "This is 0.0157% of all the words \n",
      "15 most frequent adverbs:\n",
      "only => 190\n",
      "really => 89\n",
      "certainly => 70\n",
      "immediately => 61\n",
      "perfectly => 48\n",
      "hardly => 46\n",
      "scarcely => 44\n",
      "merely => 33\n",
      "particularly => 33\n",
      "instantly => 31\n",
      "exactly => 30\n",
      "highly => 28\n",
      "exceedingly => 27\n",
      "easily => 26\n",
      "wholly => 26\n",
      "\n",
      "sonnet116.txt\n",
      "0 adverbs ending in '-ly' in total.\n",
      "This is 0.0% of all the words \n",
      "\n",
      "Ullyses.txt\n",
      "3179 adverbs ending in '-ly' in total.\n",
      "This is 0.012% of all the words \n",
      "15 most frequent adverbs:\n",
      "only => 212\n",
      "slowly => 71\n",
      "molly => 70\n",
      "simply => 57\n",
      "quickly => 54\n",
      "really => 49\n",
      "milly => 42\n",
      "suddenly => 39\n",
      "gently => 38\n",
      "lovely => 36\n",
      "probably => 34\n",
      "quietly => 33\n",
      "softly => 33\n",
      "loudly => 29\n",
      "certainly => 29\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "\n",
    "directory = os.path.join('Corpus')\n",
    "files = os.listdir(directory)\n",
    "\n",
    "for file in files:\n",
    "    print(f\"\\n{file}\")\n",
    "    path = os.path.join(directory,file)\n",
    "    \n",
    "    full_text = ''\n",
    "    with open( path , encoding = 'utf-8') as file:\n",
    "        full_text = file.read()\n",
    "\n",
    "    words = word_tokenize(full_text.lower())\n",
    "    words = remove_punctuation(words)\n",
    "    nr_words = len(words)\n",
    "    pos = pos_tag(words)\n",
    "\n",
    "    adjectives = []\n",
    "    adj_codes = ['RB','RBR','RBS']\n",
    "\n",
    "    ly_adverbs = 0\n",
    "    for p in pos:\n",
    "        if p[1] in adj_codes and p[0][-2:].strip() == 'ly':\n",
    "            adjectives.append(p[0])\n",
    "            ly_adverbs += 1\n",
    "\n",
    "    freq = Counter(adjectives)\n",
    "        \n",
    "    print(f\"{ly_adverbs} adverbs ending in '-ly' in total.\")\n",
    "    print(f\"This is {round(ly_adverbs/nr_words,4)}% of all the words \")\n",
    "    \n",
    "    number = 15\n",
    "    if ly_adverbs>0:\n",
    "        print(f\"{number} most frequent adverbs:\")\n",
    "        for word,count in freq.most_common(number):\n",
    "            print(f'{word} => {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3\n",
    "\n",
    "Which text in the corpus has the highest number of modal verbs? The Penn Treebank code for 'modal auxialiaries' is MD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BraveNewWorld.txt\n",
      "518 modal verbs.\n",
      "10 most frequent modal verbs:\n",
      "would => 116\n",
      "could => 116\n",
      "can => 68\n",
      "should => 47\n",
      "must => 43\n",
      "will => 37\n",
      "might => 34\n",
      "ought => 24\n",
      "may => 18\n",
      "shall => 15\n",
      "\n",
      "PrideandPrejudice.txt\n",
      "2892 modal verbs.\n",
      "10 most frequent modal verbs:\n",
      "could => 522\n",
      "would => 468\n",
      "will => 413\n",
      "can => 320\n",
      "must => 308\n",
      "should => 252\n",
      "might => 200\n",
      "may => 194\n",
      "shall => 162\n",
      "ought => 45\n",
      "\n",
      "sonnet116.txt\n",
      "0 modal verbs.\n",
      "\n",
      "Ullyses.txt\n",
      "2018 modal verbs.\n",
      "10 most frequent modal verbs:\n",
      "would => 382\n",
      "will => 340\n",
      "could => 309\n",
      "can => 276\n",
      "must => 221\n",
      "might => 176\n",
      "may => 91\n",
      "should => 82\n",
      "shall => 66\n",
      "ought => 61\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "\n",
    "directory = os.path.join('Corpus')\n",
    "files = os.listdir(directory)\n",
    "\n",
    "for file in files:\n",
    "    print(f\"\\n{file}\")\n",
    "    path = os.path.join(directory,file)\n",
    "    \n",
    "    full_text = ''\n",
    "    with open( path , encoding = 'utf-8') as file:\n",
    "        full_text = file.read()\n",
    "\n",
    "    words = word_tokenize(full_text.lower())\n",
    "    words = remove_punctuation(words)\n",
    "    nr_words = len(words)\n",
    "    pos = pos_tag(words)\n",
    "    \n",
    "    modal_verbs = []\n",
    "\n",
    "    for p in pos:\n",
    "        if p[1] == 'MD' and len(p[0])>2:\n",
    "            modal_verbs.append(p[0])\n",
    "\n",
    "    freq = Counter(modal_verbs)\n",
    "        \n",
    "    print(f\"{len(modal_verbs)} modal verbs.\")\n",
    "\n",
    "    number = 10\n",
    "    if len(modal_verbs)>0:\n",
    "        print(f\"{number} most frequent modal verbs:\")\n",
    "        for word,count in freq.most_common(number):\n",
    "            print(f'{word} => {count}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.4\n",
    "\n",
    "Extract all the sentences from *BraveNewWorld.txt* that contain an adjective in the superlative form.  Write these sentences into a file named 'sentences.txt'. The code for the words in these category is 'JJS'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few died; of the rest, the least susceptible divided into two; most put out four buds; some eight; all were returned to the incubators, where the buds began to develop; then, after two days, were suddenly chilled, chilled and checked. [least|most]\n",
      "From the same ovary and with gametes of the same male to manufacture as many batches of identical twins as possible--that was the best (sadly a second best) that they could do. [best]\n",
      "They could make sure of at least a hundred and fifty mature eggs within two years. [least]\n",
      "Result: they're decanted as freemartins--structurally quite normal (except,' he had to admit, 'that they do have just the slightest tendency to grow beards), but sterile. [slightest]\n",
      "'At least one glance at the Decanting Room,' he pleaded. [least]\n",
      "and his students stepped into the nearest lift and were carried up to the fifth floor. [nearest]\n",
      "NEO-PAVLOVIAN CONDITIONING ROOMS, announced the notice board. [notice]\n",
      "The swiftest crawlers were already at their goal. [swiftest]\n",
      "But, then, most historical facts are unpleasant.' [most]\n",
      "'The Nile is the longest river in Africa and the second in length of all the rivers of the globe. [longest]\n",
      "Although falling short of the length of the Mississippi-Missouri, the Nile is at the head of all rivers as regards the length of its basin, which extends through 35 degrees of latitude...'\n",
      "\n",
      "At breakfast the next morning, 'Tommy,' some one says, 'do you know which is the longest river in Africa?' [longest]\n",
      "'Although-falling-short-of...'\n",
      "\n",
      "'Well now, which is the longest river in Africa?' [longest]\n",
      "'The-Nile-is-the-longest-river-in-Africa-and-second...'\n",
      "\n",
      "'Then which river is the longest, Tommy?' [longest]\n",
      "That howl, the Director made it plain, discouraged the earliest investigators. [earliest]\n",
      "'The greatest moralizing and socializing force of all time.' [greatest]\n",
      "'I'm really awfully glad I'm a Beta, because...'\n",
      "\n",
      "Not so much like drops of water, though water, it is true, can wear holes in the hardest granite; rather, drops of liquid sealing-wax, drops that adhere, incrust, incorporate themselves with what they fall on, till finally the rock is all one scarlet blob. [hardest]\n",
      "He banged the nearest table. [nearest]\n",
      "Nowadays the Controllers won't approve of any new game unless it can be shown that it requires at least as much apparatus as the most complicated of existing games.' [least]\n",
      "'In most cases, till they were over twenty years old.' [most]\n",
      "They tried; but obviously without the smallest success. [smallest]\n",
      "A blast of warmed air dusted her with the finest talcum powder. [finest]\n",
      "The strictest conventionality.' [strictest]\n",
      "'And then he spends most of his time by himself--alone.' [most]\n",
      "****\n",
      "\n",
      "'Accompanied by a campaign against the Past; by the closing of museums, the blowing up of historical monuments (luckily most of them had already been destroyed during the Nine Years' War); by the suppression of all books published before A.F. [most]\n",
      "'You'll give me at least a week's warning, won't you,' she went on in another tone. [least]\n",
      "The chubby red face of Benito Hoover was beaming down at him--beaming with manifest cordiality. [manifest]\n",
      "Henry accelerated; the humming of the propeller shrilled from hornet to wasp, from wasp to mosquito; the speedometer showed that they were rising at the best part of two kilometres a minute. [best]\n",
      "§ 2\n",
      "\n",
      "With eyes for the most part downcast and, if ever they lighted on a fellow creature, at once and furtively averted, Bernard hastened across the roof. [most]\n",
      "He was like a man pursued, but pursued by enemies he does not wish to see, lest they should seem more hostile even than he had supposed, and he himself be made to feel guiltier and even more helplessly alone. [lest]\n",
      "Well, now she had said it and he was still wretched--wretched that she should have thought it such a perfect afternoon for Obstacle Golf, that she should have trotted away to join Henry Foster, that she should have found him funny for not wanting to talk of their most private affairs in public. [most]\n",
      "He wrote regularly for The Hourly Radio, composed feely scenarios, and had the happiest knack for slogans and hypnopædic rhymes. [happiest]\n",
      "This Escalator-Squash champion, this indefatigable lover (it was said that he had had six hundred and forty different girls in under four years), this admirable committee man and best mixer had realized quite suddenly that sport, women, communal activities were only, so far as he was concerned, second bests. [best]\n",
      "But what on earth's the good of being pierced by an article about a Community Sing, or the latest improvement in scent organs? [latest]\n",
      "Besides, can you make words really piercing--you know, like the very hardest X-rays--when you're writing about that sort of thing? [hardest]\n",
      "Which makes the best part of four hundred tons of phosphorus every year from England alone.' [best]\n",
      "And when, exhausted, the Sixteen had laid by their sexophones and the Synthetic Music apparatus was producing the very latest in slow Malthusian Blues, they might have been twin embryos gently rocking together on the waves of a bottled ocean of blood-surrogate. [latest]\n",
      "He slipped into the nearest of them as inconspicuously as he could and prepared to frown at the yet later comers whenever they should arrive. [nearest]\n",
      "If only he had given himself time to look round instead of scuttling for the nearest chair! [nearest]\n",
      "When Morgana Rothschild turned and beamed at him, he did his best to beam back. [best]\n",
      "But he went on doing his best to beam. [best]\n",
      "After a pause, sunk to a whisper, but a whisper, somehow, more penetrating than the loudest cry. [loudest]\n",
      "But he waved his arms, he shouted with the best of them; and when the others began to jig and stamp and shuffle, he also jigged and shuffled. [best]\n",
      "Moreover, for at least three days of that week they would be in the Savage Reservation. [least]\n",
      "At least Benito was normal. [least]\n",
      "Lenina did her best to stop the ears of her mind; but every now and then a phrase would insist on becoming audible. [best]\n",
      "'Oh, the greatest fun,' he answered, but in a voice so mournful, with an expression so profoundly miserable, that Lenina felt all her triumph suddenly evaporate. [greatest]\n",
      "Or at least I did. [least]\n",
      "My workers must be above suspicion, particularly those of the highest castes. [highest]\n",
      "'You don't say so,' said Lenina politely, not knowing in the least what the Warden had said, but taking her cue from his dramatic pause. [least]\n",
      "It would be the best place to spend the night. [best]\n",
      "The men came nearer and nearer; their dark eyes looked at her, but without giving any sign of recognition, any smallest sign that they had seen her or were aware of her existence. [smallest]\n",
      "Partly,' he added, 'because most of them die long before they reach this old creature's age. [most]\n",
      "Naked but for a white cotton breech-cloth, a boy of about eighteen stepped out of the crowd and stood before him, his hands crossed over his chest, his head bowed. [chest]\n",
      "****\n",
      "\n",
      "The happiest times were when she told him about the Other Place. [happiest]\n",
      "Even the longest. [longest]\n",
      "Now the world has four wombs; and he laid the seeds in the lowest of the four wombs. [lowest]\n",
      "'You see,' he said, mumbling and with averted eyes, 'I'm rather different from most people, I suppose. [most]\n",
      "It would be eighteen hours at the least before she was in time again. [least]\n",
      "'We can make a new one with the greatest ease--as many as we like. [greatest]\n",
      "For this reason I propose to dismiss him, to dismiss him with ignominy from the post he has held in this Centre; I propose forthwith to apply for his transference to a Sub-Centre of the lowest order and, that his punishment may serve the best interest of Society, as far as possible removed from any important Centre of population. [lowest|best]\n",
      "Linda, on the contrary, cut no ice; nobody had the smallest desire to see Linda. [smallest]\n",
      "Finally--and this was by far the strongest reason for people's not wanting to see poor Linda--there was her appearance. [strongest]\n",
      "So the best people were quite determined not to see Linda. [best]\n",
      "Surprisingly, as every one thought (for on soma-holiday Linda was most conveniently out of the way), John raised objections. [most]\n",
      "And if I'd had the time or the inclination, there were at least a dozen more who were only too anxious...'\n",
      "\n",
      "Helmholtz listened to his boastings in a silence so gloomily disapproving that Bernard was offended. [least]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But it was Bernard who did most of the talking. [most]\n",
      "Intoxicated, he was behaving as though, at the very least, he were a visiting World Controller. [least]\n",
      "All the best toys are kept there, and they get chocolate cream on death days. [best]\n",
      "He always does his best to avoid me; goes out of the room when I come in; won't touch me; won't even look at me. [best]\n",
      "****\n",
      "\n",
      "The scent organ was playing a delightfully refreshing Herbal Capriccio--rippling arpeggios of thyme and lavender, of rosemary, basil, myrtle, tarragon; a series of daring modulations through the spice keys into ambergris; and a slow return through sandalwood, camphor, cedar and new-mown hay (with occasional subtle touches of discord--a whiff of kidney pudding, the faintest suspicion of pig's dung) back to the simple aromatics with which the piece began. [faintest]\n",
      "Thirty or forty bars--and then, against this instrumental background, a much more than human voice began to warble; now throaty, now from the head, now hollow as a flute, now charged with yearning harmonics, it effortlessly passed from Gaspard Forster's low record on the very frontiers of musical tone to a trilled bat-note high above the highest C to which (in 1770, at the Ducal opera of Parma, and to the astonishment of Mozart) Lucrezia Ajugari, alone of all the singers in history, once piercingly gave utterance. [highest]\n",
      "He was obscurely terrified lest she should cease to be something he could feel himself unworthy of. [lest]\n",
      "What should have been the crowning moment of Bernard's whole career had turned out to be the moment of his greatest humiliation. [greatest]\n",
      "It was in the lowest spirits that he taxied across to his work at the Conditioning Centre. [lowest]\n",
      "Remember, they've had at least a quarter of a million warnings against solitude.' [least]\n",
      "'Listen to this,' was his answer; and unlocking the drawer in which he kept his mouse-eaten book, he opened and read:\n",
      "\n",
      "    'Let the bird of loudest lay,\n",
      "    On the sole Arabian tree,\n",
      "    Herald sad and trumpet be...' \n",
      "\n",
      "Helmholtz listened with a growing excitement. [loudest]\n",
      "'That old fellow,' he said, 'he makes our best propaganda technicians look absolutely silly.' [best]\n",
      "Oh, if you only knew,' he whispered, and, venturing to raise his eyes to her face, 'Admired Lenina,' he went on, 'indeed the top of admiration, worth what's dearest in the world.' [dearest]\n",
      "'Oh, you so perfect' (she was leaning towards him with parted lips), 'so perfect and so peerless are created' (nearer and nearer) 'of every creature's best.' [best]\n",
      "'The murkiest den, the most opportune place' (the voice of conscience thundered poetically), 'the strongest suggestion our worser genius can, shall never melt mine honour into lust. [murkiest|strongest]\n",
      "'The strongest oaths are straw to the fire i' the blood. [strongest]\n",
      "He had tried so hard, had done his very best; why wouldn't she allow him to forget? [best]\n",
      "'Good-bye, my dearest, dearest friends, Ford keep you! [dearest|dearest]\n",
      "Good-bye, my dearest, dearest friends. [dearest|dearest]\n",
      "Good-bye, my dearest, dearest...'\n",
      "\n",
      "When the last of the Deltas had gone the policeman switched off the current. [dearest]\n",
      "Only an Epsilon can be expected to make Epsilon sacrifices, for the good reason that for him they aren't sacrifices; they're the line of least resistance. [least]\n",
      "Shakespeare and the old men of the pueblo had never mentioned science, and from Linda he had only gathered the vaguest hints: science was something you made helicopters with, something that caused you to laugh at the Corn Dances, something that prevented you from being wrinkled and losing your teeth. [vaguest]\n",
      "Science is dangerous; we have to keep it most carefully chained and muzzled.' [most]\n",
      "'Whereas, if he had the smallest sense, he'd understand that his punishment is really a reward. [smallest]\n",
      "It has given us the stablest equilibrium in history. [stablest]\n",
      "Knowledge was the highest good, truth the supreme value; all the rest was secondary and subordinate. [highest]\n",
      "The greatest care is taken to prevent you from loving any one too much. [greatest]\n",
      "You can carry at least half your morality about in a bottle. [least]\n",
      "\"If after every tempest come such calms, may the winds blow till they have wakened death.\" [tempest]\n",
      "Most of the young men simply couldn't stand the biting and stinging. [Most]\n",
      "When morning came, he felt he had earned the right to inhabit the lighthouse: yes, even though there still was glass in most of the windows, even though the view from the platform was so fine. [most]\n",
      "Puttenham possessed no links; the nearest Riemann-surfaces were at Guildford. [nearest]\n",
      "The reporter returned his most ingratiating smile. [most]\n",
      "From his carefully constructed hide in the wood three hundred metres away, Darwin Bonaparte, the Feely Corporation's most expert big-game photographer, had watched the whole proceedings. [most]\n",
      "But now the great moment had come--the greatest, Darwin Bonaparte had time to reflect, as he moved among his instruments, the greatest since his taking of the famous all-howling stereoscopic feely of the gorillas' wedding. [greatest|greatest]\n",
      "Besides, thy best of rest is sleep, and that thou oft provok'st; yet grossly fear'st thy death which is no more. [best]\n",
      "The line wavered at its most immediately threatened point, then stiffened again, stood firm. [most]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag\n",
    "from text_mining import *\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "path = os.path.join('Corpus','BraveNewWorld.txt')\n",
    "\n",
    "with open( path , encoding = 'utf-8') as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words = remove_punctuation(words)\n",
    "    pos = pos_tag(words)\n",
    "    \n",
    "    adj = []\n",
    "    for p in pos:\n",
    "        if p[1] == 'JJS':\n",
    "            adj.append(p[0])\n",
    "            \n",
    "    if len(adj)>0:\n",
    "        print(f\"{sentence} [{'|'.join(adj)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise11.5\n",
    "\n",
    "Extract all the sentences from *Ullyses.txt* containing a form of the verb 'to see', in all tenses and conjugations and excepting the infitive form. In other words, extract sentences containing forms such as 'seen', 'saw' or 'seeing', but not 'see'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.6\n",
    "\n",
    "From *Ullyses.txt* , extract all sentnces containing the following combinations of categories: \n",
    "\n",
    "* Article - adverb - adjective - noun \n",
    "\n",
    "These categorties can be asigned the following codes:\n",
    "\n",
    "* Article: DT\n",
    "* Adverb: RB, RBR or RBS\n",
    "* Adjective: JJ, JJR or JJS\n",
    "* Noun: NN, NNP, NNPS or NNS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
