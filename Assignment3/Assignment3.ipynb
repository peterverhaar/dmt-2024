{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5e9339d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e9339d7",
        "outputId": "971af333-6d51-4ffc-d3f9-24d4ca12525c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SherlockHolmes%20(1).txt\n"
          ]
        }
      ],
      "source": [
        "from os.path import basename\n",
        "\n",
        "my_text = 'https://raw.githubusercontent.com/peterverhaar/dmt-2024/refs/heads/main/Vallesi/SherlockHolmes%20(1).txt'\n",
        "file_name = basename(my_text)\n",
        "print(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f93899f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f93899f6",
        "outputId": "28a08403-6cd5-453a-a32f-f7497859f9fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (4.25.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.6)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from stanza) (2.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
            "Downloading stanza-1.9.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.14.0 stanza-1.9.2\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!pip install stanza\n",
        "!pip install vaderSentiment\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "import requests\n",
        "from os.path import basename\n",
        "\n",
        "def download(url):\n",
        "    response = requests.get(url)\n",
        "    if response:\n",
        "        file_name = basename(url)\n",
        "        out = open(file_name,'w',encoding='utf-8')\n",
        "        out.write(response.text)\n",
        "        out.close()\n",
        "\n",
        "def sorted_by_value( dict , ascending = True ):\n",
        "    if ascending:\n",
        "        return {k: v for k, v in sorted(dict.items(), key=lambda item: item[1])}\n",
        "    else:\n",
        "        return {k: v for k, v in reversed( sorted(dict.items(), key=lambda item: item[1]))}\n",
        "\n",
        "download('https://raw.githubusercontent.com/peterverhaar/dmt-2024-assignment3/refs/heads/main/Assignment3.ipynb')\n",
        "download('https://raw.githubusercontent.com/peterverhaar/dmt-2024-assignment3/refs/heads/main/text_mining.py')\n",
        "download(my_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ab22f221",
      "metadata": {
        "id": "ab22f221"
      },
      "outputs": [],
      "source": [
        "from text_mining import *\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.text import Text\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import stanza\n",
        "import json\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1518b1b3",
      "metadata": {
        "id": "1518b1b3"
      },
      "outputs": [],
      "source": [
        "text_file = open(file_name,encoding='utf-8')\n",
        "full_text = text_file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "15f9409f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15f9409f",
        "outputId": "de7827d4-c54f-4378-daf3-2b1021d98d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The novel contains 8889 words\n"
          ]
        }
      ],
      "source": [
        "# Calculate number of words\n",
        "words = word_tokenize(full_text.lower())\n",
        "words = remove_punctuation(words)\n",
        "nr_tokens = len(words)\n",
        "\n",
        "print(f'The novel contains {nr_tokens} words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fb9dbf37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb9dbf37",
        "outputId": "eb2d3ba9-9bb5-4b6f-a871-7e8ccbe7902f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The novel contains 1819 unique words\n"
          ]
        }
      ],
      "source": [
        "word_frequencies = Counter(words)\n",
        "print(f'The novel contains {len(word_frequencies)} unique words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "391f8112",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "391f8112",
        "outputId": "1a294c57-2509-422a-e810-c5399ba95e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following 20 words are most common:\n",
            "the => 461\n",
            "and => 277\n",
            "i => 263\n",
            "a => 237\n",
            "of => 224\n",
            "to => 218\n",
            "that => 157\n",
            "in => 147\n",
            "he => 145\n",
            "you => 139\n",
            "it => 137\n",
            "was => 107\n",
            "is => 107\n",
            "his => 104\n",
            "as => 92\n",
            "have => 88\n",
            "for => 85\n",
            "my => 76\n",
            "with => 76\n",
            "we => 63\n"
          ]
        }
      ],
      "source": [
        "nr_words = 20\n",
        "\n",
        "print(f'The following {nr_words} words are most common:')\n",
        "\n",
        "for word,count in word_frequencies.most_common(nr_words):\n",
        "    print(f\"{word} => {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1ac1c09f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ac1c09f",
        "outputId": "dd675eef-0bc6-441c-83aa-806e10f77c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When we remove the stopwords, the following 20 words are most common:\n",
            "holmes 53\n",
            "said 51\n",
            "upon 49\n",
            "would 35\n",
            "one 30\n",
            "little 25\n",
            "man 25\n",
            "well 25\n",
            "could 23\n",
            "wilson 22\n",
            "see 22\n",
            "us 19\n",
            "time 18\n",
            "come 17\n",
            "must 17\n",
            "might 17\n",
            "business 17\n",
            "never 16\n",
            "right 15\n",
            "good 15\n",
            "heard 15\n",
            "two 15\n",
            "league 14\n",
            "know 14\n",
            "every 14\n",
            "assistant 14\n",
            "nothing 13\n",
            "asked 13\n",
            "first 13\n",
            "answered 13\n"
          ]
        }
      ],
      "source": [
        "nr_words = 20\n",
        "\n",
        "words = remove_punctuation_and_stopwords(words)\n",
        "word_frequencies = Counter(words)\n",
        "\n",
        "print(f'When we remove the stopwords, the following {nr_words} words are most common:')\n",
        "\n",
        "for word,count in word_frequencies.most_common(30):\n",
        "    print(f\"{word} {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "849f990e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "849f990e",
        "outputId": "f740706f-6e17-45e0-deab-db228e8c2557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The novel contains 395 sentences.\n",
            "The sentences contain 22.5 words on average\n"
          ]
        }
      ],
      "source": [
        "sentences = sent_tokenize(full_text)\n",
        "print(f'The novel contains {len(sentences)} sentences.')\n",
        "avg_nr_words = round((nr_tokens/len(sentences)),2)\n",
        "print(f'The sentences contain {avg_nr_words} words on average')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b25de79c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b25de79c",
        "outputId": "6e69bcf9-92aa-4e8d-d90e-6cfdaaa2a7a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 1 of 1 matches:\n",
            "e upon brilliant reasoning power would rise level intuition\n"
          ]
        }
      ],
      "source": [
        "novel = Text(words)\n",
        "novel.concordance('power' , width = 60 , lines = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8d8377d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d8377d2",
        "outputId": "3908fe9e-a7b2-4c5d-9c71-8de8e5504fbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "would 2\n",
            "suddenly 1\n",
            "come 1\n",
            "upon 1\n",
            "him 1\n",
            "and 1\n",
            "that 1\n",
            "his 1\n",
            "brilliant 1\n",
            "reasoning 1\n",
            "rise 1\n",
            "to 1\n",
            "the 1\n",
            "level 1\n",
            "of 1\n",
            "intuition 1\n",
            "until 1\n",
            "those 1\n"
          ]
        }
      ],
      "source": [
        "freq = collocation( full_text , r'power' , 20)\n",
        "\n",
        "for nearby_word in freq:\n",
        "    print(nearby_word , freq[nearby_word])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7adb77b1",
      "metadata": {
        "id": "7adb77b1"
      },
      "outputs": [],
      "source": [
        "adjective_codes = ['JJ','JJR','JJS']\n",
        "adverb_codes = ['RB','RBR','RBS']\n",
        "\n",
        "adjectives = Counter()\n",
        "adverbs = Counter()\n",
        "\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    words = remove_punctuation(words)\n",
        "\n",
        "    pos = pos_tag(words)\n",
        "\n",
        "    for p in pos:\n",
        "\n",
        "        if p[1] in adjective_codes:\n",
        "            adjectives.update([p[0]])\n",
        "\n",
        "        if p[1] in adverb_codes:\n",
        "            adverbs.update([p[0]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "194da397",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "194da397",
        "outputId": "fb4d577c-0163-4c73-d65d-8376bc4a24ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('little', 25)\n",
            "('other', 16)\n",
            "('good', 15)\n",
            "('first', 10)\n",
            "('small', 9)\n",
            "('right', 9)\n",
            "('such', 9)\n",
            "('many', 8)\n",
            "('whole', 8)\n",
            "('red', 7)\n"
          ]
        }
      ],
      "source": [
        "for word in adjectives.most_common(10):\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "233efb4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "233efb4e",
        "outputId": "2b16e519-4310-40e0-cf90-e28bff36ae22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('not', 59)\n",
            "('very', 39)\n",
            "('so', 30)\n",
            "('Then', 16)\n",
            "('then', 16)\n",
            "('never', 15)\n",
            "('just', 14)\n",
            "('as', 14)\n",
            "('most', 12)\n",
            "('only', 11)\n"
          ]
        }
      ],
      "source": [
        "for word in adverbs.most_common(10):\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "79849b4b",
      "metadata": {
        "id": "79849b4b"
      },
      "outputs": [],
      "source": [
        "ana = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "fa87813b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa87813b",
        "outputId": "f2719248-0e0a-4bf5-ee5b-1848639da9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive words:\n",
            "well\n",
            "good\n",
            "hand\n",
            "yes\n",
            "matter\n",
            "like\n",
            "friend\n",
            "better\n",
            "dear\n",
            "hope\n",
            "daring\n",
            "great\n",
            "remarkable\n",
            "ready\n",
            "excuse\n",
            "wish\n",
            "sure\n",
            "help\n",
            "want\n",
            "play\n",
            "\n",
            "Negative words:\n",
            "no\n",
            "doubt\n",
            "pay\n",
            "problem\n",
            "miss\n",
            "crime\n",
            "lose\n",
            "difficult\n",
            "stopped\n",
            "fiery\n",
            "questioning\n",
            "bizarre\n",
            "strange\n",
            "trick\n",
            "poor\n",
            "hard\n",
            "fault\n",
            "despair\n",
            "serious\n",
            "leave\n"
          ]
        }
      ],
      "source": [
        "positive_words = []\n",
        "negative_words = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    for word in words:\n",
        "        scores = ana.polarity_scores(word)\n",
        "        if scores[\"pos\"] > 0.75:\n",
        "            positive_words.append(word)\n",
        "        elif scores[\"neg\"] > 0.75:\n",
        "            negative_words.append(word)\n",
        "\n",
        "print('Positive words:')\n",
        "\n",
        "sentiment_freq = Counter(positive_words)\n",
        "\n",
        "for word,count in sentiment_freq.most_common(20):\n",
        "    print(word)\n",
        "\n",
        "\n",
        "print('\\nNegative words:')\n",
        "\n",
        "sentiment_freq = Counter(negative_words)\n",
        "\n",
        "for word,count in sentiment_freq.most_common(20):\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "93c6e6fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93c6e6fa",
        "outputId": "f73b4057-c937-405f-8f47-2199f683e8c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Postive sentences\n",
            "\n",
            "“I have been at some small expense over this matter, which I shall expect the bank to refund, but beyond that I am amply repaid by having had an experience which is in many ways unique, and by hearing the very remarkable narrative of the Red-headed League.” “You see, Watson,” he explained in the early hours of the morning as we sat over a glass of whisky and soda in Baker Street, “it was perfectly obvious from the first that the only possible object of this rather fantastic business of the advertisement of the League, and the copying of the _Encyclopædia_, must be to get this not over-bright pawnbroker out of the way for a number of hours every day. [0.9615]\n",
            "Now, if you cared to apply, Mr. Wilson, you would just walk in; but perhaps it would hardly be worth your while to put yourself out of the way for the sake of a few hundred pounds.’ “Now, it is a fact, gentlemen, as you may see for yourselves, that my hair is of a very full and rich tint, so that it seemed to me that if there was to be any competition in the matter I stood as good a chance as any man that I had ever met. [0.9396]\n",
            "All the afternoon he sat in the stalls wrapped in the most perfect happiness, gently waving his long, thin fingers in time to the music, while his gently smiling face and his languid, dreamy eyes were as unlike those of Holmes the sleuth-hound, Holmes the relentless, keen-witted, ready-handed criminal agent, as it was possible to conceive. [0.9317]\n",
            "“‘Then, good-bye, Mr. Jabez Wilson, and let me congratulate you once more on the important position which you have been fortunate enough to gain.’ He bowed me out of the room and I went home with my assistant, hardly knowing what to say or do, I was so pleased at my own good fortune. [0.9301]\n",
            "This gentleman, Mr. Wilson, has been my partner and helper in many of my most successful cases, and I have no doubt that he will be of the utmost use to me in yours also.” The stout gentleman half rose from his chair and gave a bob of greeting, with a quick little questioning glance from his small fat-encircled eyes. [0.8956]\n",
            "You have shown your relish for it by the enthusiasm which has prompted you to chronicle, and, if you will excuse my saying so, somewhat to embellish so many of my own little adventures.” “Your cases have indeed been of the greatest interest to me,” I observed. [0.886]\n",
            "There was a double stream upon the stair, some going up in hope, and some coming back dejected; but we wedged in as well as we could and soon found ourselves in the office.” “Your experience has been a most entertaining one,” remarked Holmes as his client paused and refreshed his memory with a huge pinch of snuff. [0.867]\n",
            "Then suddenly he plunged forward, wrung my hand, and congratulated me warmly on my success. [0.8625]\n",
            "He was himself red-headed, and he had a great sympathy for all red-headed men; so, when he died, it was found that he had left his enormous fortune in the hands of trustees, with instructions to apply the interest to the providing of easy berths to men whose hair is of that colour. [0.8481]\n",
            "The swing of his nature took him from extreme languor to devouring energy; and, as I knew well, he was never so truly formidable as when, for days on end, he had been lounging in his armchair amid his improvisations and his black-letter editions. [0.8237]\n",
            "\n",
            "Negative sentences\n",
            "\n",
            "“If you can do nothing better than laugh at me, I can go elsewhere.” “No, no,” cried Holmes, shoving him back into the chair from which he had half risen. [-0.7864]\n",
            "I trust that I am not more dense than my neighbours, but I was always oppressed with a sense of my own stupidity in my dealings with Sherlock Holmes. [-0.7814]\n",
            "‘_Omne ignotum pro magnifico_,’ you know, and my poor little reputation, such as it is, will suffer shipwreck if I am so candid. [-0.765]\n",
            "To me, with my nerves worked up to a pitch of expectancy, there was something depressing and subduing in the sudden gloom, and in the cold dank air of the vault. [-0.765]\n",
            "For you, Mr. Merryweather, the stake will be some £ 30,000; and for you, Jones, it will be the man upon whom you wish to lay your hands.” “John Clay, the murderer, thief, smasher, and forger. [-0.743]\n",
            "I don’t know that your assistant is not as remarkable as your advertisement.” “Oh, he has his faults, too,” said Mr. Wilson. [-0.7205]\n",
            "He’s a young man, Mr. Merryweather, but he is at the head of his profession, and I would rather have my bracelets on him than on any criminal in London. [-0.6808]\n",
            "“Why, I have lost four pound a week.” “As far as you are personally concerned,” remarked Holmes, “I do not see that you have any grievance against this extraordinary league. [-0.6597]\n",
            "When I saw him that afternoon so enwrapped in the music at St. James’s Hall I felt that an evil time might be coming upon those whom he had set himself to hunt down. [-0.6597]\n",
            "This business at Coburg Square is serious.” “Why serious?” “A considerable crime is in contemplation. [-0.5849]\n"
          ]
        }
      ],
      "source": [
        "sent_scores = dict()\n",
        "\n",
        "sentences = [re.sub('\\n+', ' ' , sent) for sent in sentences]\n",
        "\n",
        "for s in sentences:\n",
        "    scores = ana.polarity_scores(s)\n",
        "    sent_scores[s] = scores['compound']\n",
        "\n",
        "nr_sentences = 10\n",
        "\n",
        "i = 0\n",
        "\n",
        "print('\\nPostive sentences\\n')\n",
        "\n",
        "for s in sorted_by_value( sent_scores , ascending = False ):\n",
        "    print( f'{s} [{ sent_scores[s]}]' )\n",
        "    i+= 1\n",
        "    if i == nr_sentences:\n",
        "        break\n",
        "\n",
        "print('\\nNegative sentences\\n')\n",
        "i = 0\n",
        "\n",
        "for s in sorted_by_value( sent_scores , ascending = True):\n",
        "    print( f'{s} [{ sent_scores[s]}]' )\n",
        "    i+= 1\n",
        "    if i == nr_sentences:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3d5a9f31",
      "metadata": {
        "id": "3d5a9f31"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def wordnet_hypernyms(token):\n",
        "    all_hypernyms = []\n",
        "    black_list = ['queen','young','human']\n",
        "\n",
        "    if token not in black_list:\n",
        "\n",
        "        word_senses = wn.synsets(token)\n",
        "        hypernyms = lambda s: s.hypernyms()\n",
        "        return_value = False\n",
        "        for ws in word_senses:\n",
        "\n",
        "            hypernyms = [hyp.name() for hyp in list(ws.closure(hypernyms))]\n",
        "            if 'plant.n.02' in hypernyms:\n",
        "                all_hypernyms.append('plant')\n",
        "            if 'color.n.01' in hypernyms:\n",
        "                all_hypernyms.append('colour')\n",
        "            if 'emotion.n.01' in hypernyms:\n",
        "                all_hypernyms.append('emotion')\n",
        "            if 'animal.n.01' in hypernyms:\n",
        "                all_hypernyms.append('animal')\n",
        "            if 'natural_phenomenon.n.01' in hypernyms:\n",
        "                all_hypernyms.append('natural_phenomenon')\n",
        "            if 'body_part.n.01' in hypernyms:\n",
        "                all_hypernyms.append('body_part')\n",
        "\n",
        "    return all_hypernyms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "029ba9d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "029ba9d6",
        "outputId": "48d1ae93-a740-41bb-cb13-08ca7f0f1e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common emotions:\n",
            "love\n",
            "grievance\n",
            "excitement\n",
            "happiness\n",
            "misgivings\n",
            "\n",
            "Most common colours:\n",
            "red\n",
            "black\n",
            "brown\n",
            "tint\n",
            "drab\n",
            "pink\n",
            "yellow\n",
            "\n",
            "Most common body parts:\n",
            "head\n",
            "small\n",
            "face\n",
            "behind\n",
            "hand\n",
            "back\n",
            "knees\n",
            "quick\n",
            "fingers\n",
            "knee\n",
            "wrist\n",
            "ears\n",
            "lips\n",
            "chest\n",
            "eye\n",
            "\n",
            "Most common natural phenomena:\n",
            "light\n",
            "thrust\n",
            "low\n",
            "smoke\n",
            "energy\n",
            "quiet\n"
          ]
        }
      ],
      "source": [
        "word_list = []\n",
        "\n",
        "for word,count in word_frequencies.most_common():\n",
        "    hypernyms = wordnet_hypernyms(word)\n",
        "    if len(hypernyms)>0:\n",
        "        for h in hypernyms:\n",
        "            word_list.append((word,h))\n",
        "\n",
        "def find_most_common_hyponyms(domain):\n",
        "    common_hyponyms = []\n",
        "    for word, hyponym in word_list:\n",
        "        if hyponym == domain:\n",
        "            common_hyponyms.append(word)\n",
        "    return Counter(common_hyponyms)\n",
        "\n",
        "print(f\"Most common emotions:\")\n",
        "common_hyponyms = find_most_common_hyponyms('emotion')\n",
        "for word,count in common_hyponyms.most_common(15):\n",
        "    print(word)\n",
        "\n",
        "print(f\"\\nMost common colours:\")\n",
        "common_hyponyms = find_most_common_hyponyms('colour')\n",
        "for word,count in common_hyponyms.most_common(15):\n",
        "    print(word)\n",
        "\n",
        "print(f\"\\nMost common body parts:\")\n",
        "common_hyponyms = find_most_common_hyponyms('body_part')\n",
        "for word,count in common_hyponyms.most_common(15):\n",
        "    print(word)\n",
        "\n",
        "common_hyponyms = find_most_common_hyponyms('natural_phenomenon')\n",
        "if len(common_hyponyms)>0:\n",
        "    print(f\"\\nMost common natural phenomena:\")\n",
        "    for word,count in common_hyponyms.most_common(15):\n",
        "        print(word)\n",
        "\n",
        "\n",
        "common_hyponyms = find_most_common_hyponyms('amimal')\n",
        "if len(common_hyponyms)>0:\n",
        "    print(f\"\\nMost common animals:\")\n",
        "    for word,count in common_hyponyms.most_common(15):\n",
        "        print(word)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}